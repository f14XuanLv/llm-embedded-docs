<h1>在安卓手机上部署多模态llm与模型转换&推理优化</h1>

<h2>部署纯文本llm</h2>

<p class="model-subtitle">以gemma-3-1b-it-q4_k.gguf为例</p>

<h3>安装 Termux</h3>

<p>Termux是一个Android终端模拟器和Linux环境应用程序，它允许用户在Android设备上运行Linux命令行工具和应用程序。
Termux提供了一个强大的包管理器，可以安装各种软件包和工具。
其次，Termux还支持SSH服务，可以让用户通过SSH远程访问和管理设备。</p>

<h4>下载安装Termux</h4>

<p>先去 <a href="https://github.com/termux/termux-app/releases" target="_blank">https://github.com/termux/termux-app/releases</a> 下载最新的Termux安装包
安装完成后，打开Termux应用程序。</p>

<p><img src="images/Termux_phone0.jpg" alt="Termux界面" style="max-width: 40%; border-radius: 12px; box-shadow: 0 4px 8px rgba(0,0,0,0.2);"></p>

<h4>初始配置Termux</h4>

<pre><code class="language-bash">
# 更新包管理器
apt update && apt upgrade -y

# 安装必要的工具
apt install openssh git cmake python -y

# 允许Termux访问手机存储
termux-setup-storage
</code></pre>

<h4>设置SSH以便从电脑连接</h4>

<pre><code class="language-bash">
# 设置密码（用于SSH登录）
passwd

# 启动SSH服务
sshd

# 查看手机IP地址
ip addr show | grep inet

# 查看用户名
whoami
</code></pre>

<p>要记录下来的信息：<br>
IP地址（通常是192.168.x.x或10.x.x.x格式）<br>
用户名（通常是"u0_a"开头的字符串）。</p>

<h4>从电脑连接到Termux</h4>

<p>使用远程工具（如MobaXterm或VSCode的SSH扩展）连接到Termux，使用
用户名和密码登录。</p>

<p><img src="images/Termux_MobaXterm.png" alt="MobaXterm连接" style="max-width: 80%; border-radius: 12px; box-shadow: 0 4px 8px rgba(0,0,0,0.2);"></p>

<p>手机上的Termux退出后，如果下次要再连接，需要重新启动SSH服务，并且查看IP地址是否与之前相同：</p>

<pre><code class="language-bash">
sshd

ip addr show | grep inet
</code></pre>

<h3>克隆并编译llama.cpp</h3>

<h4>克隆llama.cpp</h4>

<pre><code class="language-bash">
git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp
</code></pre>

<p>一般情况下如果要从Github上克隆代码，可能手机需要使用网络代理，
也可以电脑上下载，然后通过ssh软件的文件传输功能，将项目传输到手机上。</p>

<h4>编译llama.cpp</h4>

<pre><code class="language-bash">
mkdir build
cd build
cmake ..
make -j8 # 使用8个线程编译
</code></pre>

<h3>下载模型</h3>

<p>推荐直接去
<a href="https://huggingface.co/Mungert/gemma-3-1b-it-gguf/tree/main" target="_blank">https://huggingface.co/Mungert/gemma-3-1b-it-gguf/tree/main</a>
下载</p>

<p><img src="images/Download_gemma-3-1b-it-gguf.png" alt="下载模型" style="max-width: 80%; border-radius: 12px; box-shadow: 0 4px 8px rgba(0,0,0,0.2);"></p>

<p>google_gemma-3-1b-it-q4_k.gguf<br>
然后通过ssh传输，将其放置在 llama.cpp/models/ 目录下</p>

<p>或者使用huggingface-cli下载模型：</p>

<pre><code class="language-bash">
cd .. # 返回到 llama.cpp 目录下

# 使用huggingface-cli下载模型，
# 需要注册huggingface账号和安装huggingface-cli，并配置令牌
huggingface-cli download Mungert/gemma-3-1b-it-gguf google_gemma-3-1b-it-q4_k.gguf --local-dir ./models
</code></pre>

<h3>运行模型</h3>

<pre><code class="language-bash">
./build/bin/llama-cli -m ./models/google_gemma-3-1b-it-q4_k.gguf
</code></pre>

<p><img src="images/RunningLLM0.png" alt="运行模型" style="max-width: 80%; border-radius: 12px; box-shadow: 0 4px 8px rgba(0,0,0,0.2);"></p>

<p>之后正常对话即可</p>

<h2>部署图生文llm</h2>

<p>Gemma3 支持图生文的最小的模型是 Gemma3-4B，以此为例</p>

<h3>测试安卓手机上运行Gemma3-4B多模态的效果</h3>

<h4>首先下载 Gemma3-4B的模型文件</h4>

<p>进入 <a href="https://huggingface.co/Mungert/gemma-3-4b-it-gguf/tree/main" target="_blank">https://huggingface.co/Mungert/gemma-3-4b-it-gguf/tree/main</a></p>

<p>选择两个 Gemma3-4B 中轻量的文本模型和图像嵌入模型，<br>
分别是：<br>
gemma-3-4b-it-q4_k_s.gguf<br>
google_gemma-3-4b-it-mmproj-q8.gguf<br>
下载后使用 ssh 传输到手机的 llama.cpp/models/ 目录下</p>

<p><img src="images/normal_and_mmproj.png" alt="模型文件" style="max-width: 80%; border-radius: 12px; box-shadow: 0 4px 8px rgba(0,0,0,0.2);"></p>

<p>也可以使用 huggingface-cli 下载模型：<br>
下载到 手机的 llama.cpp/models/ 目录下，</p>

<h4>然后运行模型</h4>

<pre><code class="language-bash">
cd llama.cpp # 进入 llama.cpp 目录

./build/bin/llama-gemma3-cli \
    -m models/google_gemma-3-4b-it-q4_k_s.gguf \
    --mmproj models/google_gemma-3-4b-it-mmproj-q8.gguf
</code></pre>

<p>运行成功，提示输入图片的指令</p>
<p><img src="images/image_input_tips.png" alt="输入图片提示" style="max-width: 80%; border-radius: 12px; box-shadow: 0 4px 8px rgba(0,0,0,0.2);"></p>

<p>在特定目录准备一张小图片，<br>
我将测试图放在了 /data/data/com.termux/files/home/llama.cpp/media/test001.png</p>

<p>这是一张很小的图片，仅48*48</p>

<p>输入图片，路径最好是termux下的绝对路径，</p>
<pre><code class="language-bash">/image /data/data/com.termux/files/home/llama.cpp/media/test001.png</code></pre>

<p><img src="images/mmproj_test0.png" alt="模型测试" style="max-width: 80%; border-radius: 12px; box-shadow: 0 4px 8px rgba(0,0,0,0.2);"></p>

<p>测试结果：可以成功识别，但是图片编码和解码非常慢<br>
Image encoded in 80559 ms<br>
Image decoded in 18534 ms</p>

<h2>模型转换&推理优化</h2>

<h3>demo</h3>

<p>由于我的手机芯片是高通骁龙870，</p>

<p>高通官方的神经网络推理框架提供了SNPE (Snapdragon Neural Processing Engine) ，专门为骁龙芯片优化，能够充分利用870的AI引擎、GPU和CPU。</p>

<p>所以预计技术路线如下：<br>
Hugging Face (PyTorch兼容的 safetensors 格式) → ONNX → SNPE DLC → 量化DLC → 手机部署</p>

<pre><code class="language-bash">
huggingface-cli login

python -c "from huggingface_hub import snapshot_download; snapshot_download('google/gemma-3-4b-it', local_dir='./gemma-3-4b-it')"
</code></pre>

<p>后续内容待补充</p>

<h2>附录</h2>

<h3>模型文件名相关后缀的说明</h3>

<h4>浮点精度</h4>
<ul>
    <li><strong>fp16/f16</strong>：16位浮点精度，相比32位浮点精度(fp32)节省内存，同时在支持fp16加速的硬件上能获得更好的性能。适合GPU推理。</li>
    <li><strong>bf16</strong>：Brain浮点格式，是一种特殊的16位浮点格式。与fp16相比，它保留了与fp32相同的指数范围，但精度降低。适合支持bf16加速的现代GPU和TPU设备，能在降低内存使用的同时保持较高的性能和稳定性。</li>
</ul>

<h4>量化精度</h4>
<ul>
    <li><strong>q8/Q8_0</strong>：将模型权重量化为8位整数，是量化模型中精度最高的常见选项，但也消耗更多内存。</li>
    <li><strong>q6_k/Q6_K</strong>：使用6位量化技术，k代表"k-quants"量化方法。在精度和内存占用之间提供平衡。</li>
    <li><strong>q4_k/Q4_K</strong>：4位量化，提供更小的模型体积，适合内存受限环境，但精度略低。</li>
    <li><strong>q3/Q3_K</strong>：3位量化，提供最小的模型体积，但精度相对较低。</li>
</ul>

<h4>大小和速度指标</h4>
<ul>
    <li><strong>l/L</strong>：Large的缩写，通常指保留更多细节的量化模型，文件更大但精度更高。</li>
    <li><strong>m/M</strong>：Medium的缩写，指中等大小的量化模型，平衡了精度和大小。</li>
    <li><strong>s/S</strong>：Small的缩写，指更轻量级的量化模型，体积最小但可能在精度上有所牺牲。</li>
</ul>

<h3>模型参数调用表</h3>

<p>以下是运行 Gemma 模型时可使用的主要参数，包括调用方法、意义及默认值：</p>

<table>
    <thead>
        <tr>
            <th>参数</th>
            <th>命令行选项</th>
            <th>意义</th>
            <th>默认值</th>
            <th>推荐值/范围</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>模型路径</td>
            <td><code>-m, --model</code></td>
            <td>指定模型文件路径</td>
            <td>无</td>
            <td>需指定具体路径</td>
        </tr>
        <tr>
            <td>线程数</td>
            <td><code>-t, --threads</code></td>
            <td>指定使用的CPU线程数</td>
            <td>系统最大线程数</td>
            <td>4-8 (取决于CPU核心数)</td>
        </tr>
        <tr>
            <td>上下文大小</td>
            <td><code>--ctx-size</code></td>
            <td>模型能处理的最大上下文长度</td>
            <td>512</td>
            <td>2048-32768</td>
        </tr>
        <tr>
            <td>最大生成长度</td>
            <td><code>-n, --n-predict</code></td>
            <td>模型单次最多生成的token数</td>
            <td>128</td>
            <td>256-2048</td>
        </tr>
        <tr>
            <td>温度</td>
            <td><code>--temp</code></td>
            <td>控制生成文本的随机性</td>
            <td>0.8</td>
            <td>0-2.0</td>
        </tr>
        <tr>
            <td>Top-K</td>
            <td><code>--top-k</code></td>
            <td>每步考虑的最高概率token数</td>
            <td>40</td>
            <td>0-100</td>
        </tr>
        <tr>
            <td>Top-P</td>
            <td><code>--top-p</code></td>
            <td>累积概率阈值筛选</td>
            <td>0.9</td>
            <td>0-1.0</td>
        </tr>
        <tr>
            <td>批处理大小</td>
            <td><code>--batch-size</code></td>
            <td>并行处理的token数量</td>
            <td>512</td>
            <td>32-2048</td>
        </tr>
        <tr>
            <td>重复惩罚</td>
            <td><code>--repeat-penalty</code></td>
            <td>控制文本重复的惩罚系数</td>
            <td>1.1</td>
            <td>1.0-2.0</td>
        </tr>
        <tr>
            <td>频率惩罚</td>
            <td><code>--freq-penalty</code></td>
            <td>惩罚频繁出现的token</td>
            <td>0.0</td>
            <td>0-2.0</td>
        </tr>
        <tr>
            <td>存在惩罚</td>
            <td><code>--presence-penalty</code></td>
            <td>惩罚已出现过的token</td>
            <td>0.0</td>
            <td>0-2.0</td>
        </tr>
        <tr>
            <td>随机种子</td>
            <td><code>--seed</code></td>
            <td>控制随机生成的确定性</td>
            <td>-1 (随机)</td>
            <td>任意整数</td>
        </tr>
        <tr>
            <td>提示词</td>
            <td><code>--prompt</code></td>
            <td>指定初始提示词</td>
            <td>无</td>
            <td>任意文本</td>
        </tr>
        <tr>
            <td>交互模式</td>
            <td><code>-i, --interactive</code></td>
            <td>启用交互式聊天模式</td>
            <td>启用</td>
            <td>-</td>
        </tr>
        <tr>
            <td>多行输入</td>
            <td><code>--multiline-input</code></td>
            <td>允许在交互模式下输入多行</td>
            <td>未启用</td>
            <td>-</td>
        </tr>
        <tr>
            <td>记录提示词</td>
            <td><code>--prompt-cache</code></td>
            <td>缓存提示词的文件路径</td>
            <td>未启用</td>
            <td>文件路径</td>
        </tr>
        <tr>
            <td>记录输出</td>
            <td><code>--log-file</code></td>
            <td>记录输出内容的文件路径</td>
            <td>未启用</td>
            <td>文件路径</td>
        </tr>
        <tr>
            <td>Mirostat采样</td>
            <td><code>--mirostat</code></td>
            <td>启用Mirostat采样算法</td>
            <td>0 (关闭)</td>
            <td>0-2</td>
        </tr>
        <tr>
            <td>KV缓存</td>
            <td><code>--no-kv-cache</code></td>
            <td>禁用KV缓存加速生成</td>
            <td>启用</td>
            <td>-</td>
        </tr>
        <tr>
            <td>内存映射</td>
            <td><code>--mmap</code></td>
            <td>使用内存映射加载模型</td>
            <td>启用</td>
            <td>-</td>
        </tr>
        <tr>
            <td>模型加载模式</td>
            <td><code>--lora</code></td>
            <td>加载LoRA适配器</td>
            <td>未启用</td>
            <td>文件路径</td>
        </tr>
        <tr>
            <td>语言</td>
            <td><code>--language</code></td>
            <td>指定响应语言</td>
            <td>自动</td>
            <td>"zh"(中文),"en"(英文)等</td>
        </tr>
        <tr>
            <td>系统提示词</td>
            <td><code>--system-prompt</code></td>
            <td>设置系统提示词</td>
            <td>无</td>
            <td>任意文本</td>
        </tr>
        <tr>
            <td>GPU层数</td>
            <td><code>-ngl, --n-gpu-layers</code></td>
            <td>在GPU上运行的层数</td>
            <td>0</td>
            <td>0-全部层数</td>
        </tr>
    </tbody>
</table>

<h3>参数详细说明</h3>

<h4>基本参数</h4>
<ul>
    <li><strong>模型路径</strong>：必须参数，指定模型文件位置</li>
    <li><strong>线程数</strong>：影响推理速度，建议设置为实际CPU核心数或稍低</li>
    <li><strong>上下文大小</strong>：影响模型能"记住"的内容长度，更大的值需要更多内存</li>
</ul>

<h4>生成控制参数</h4>
<ul>
    <li><strong>温度</strong>：
        <ul>
            <li>0：完全确定性输出，总是选择最可能的token</li>
            <li>0.7：平衡创造性与一致性</li>
            <li>1.0+：更加随机和创造性的输出</li>
        </ul>
    </li>
    <li><strong>Top-K</strong>：每步考虑概率最高的K个token
        <ul>
            <li>较小值(10)：更保守的输出</li>
            <li>较大值(100)：更多样化的输出</li>
            <li>0：禁用Top-K筛选</li>
        </ul>
    </li>
    <li><strong>Top-P</strong>：累积概率达到P时截止考虑候选token
        <ul>
            <li>较小值(0.5)：更保守、可预测的输出</li>
            <li>较大值(0.95)：更多样化的输出</li>
        </ul>
    </li>
</ul>

<h4>性能参数</h4>
<ul>
    <li><strong>批处理大小</strong>：并行处理token数量，更大值提高吞吐量但增加内存占用</li>
    <li><strong>内存映射</strong>：通过映射文件而非加载整个模型到内存来减少RAM使用</li>
    <li><strong>GPU层数</strong>：指定多少层在GPU上运行，0表示完全使用CPU</li>
</ul>

<h4>特殊应用场景的参数设置</h4>

<table>
    <thead>
        <tr>
            <th>场景</th>
            <th>温度</th>
            <th>Top-P</th>
            <th>Top-K</th>
            <th>重复惩罚</th>
            <th>其他参数</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>创意写作</td>
            <td>0.8-1.0</td>
            <td>0.95</td>
            <td>60</td>
            <td>1.1</td>
            <td>较大生成长度</td>
        </tr>
        <tr>
            <td>严谨问答</td>
            <td>0.3-0.5</td>
            <td>0.8</td>
            <td>30</td>
            <td>1.2</td>
            <td>-</td>
        </tr>
        <tr>
            <td>代码生成</td>
            <td>0.2-0.4</td>
            <td>0.9</td>
            <td>20</td>
            <td>1.3-1.5</td>
            <td>频率惩罚=0.1</td>
        </tr>
        <tr>
            <td>故事续写</td>
            <td>0.7-0.9</td>
            <td>0.92</td>
            <td>50</td>
            <td>1.05</td>
            <td>-</td>
        </tr>
        <tr>
            <td>摘要生成</td>
            <td>0.4-0.6</td>
            <td>0.85</td>
            <td>40</td>
            <td>1.2</td>
            <td>-</td>
        </tr>
    </tbody>
</table>

<h3>命令示例</h3>

<pre><code class="language-bash">
./build/bin/llama-cli -m ./models/google_gemma-3-1b-it-q4_k.gguf \
  --threads 4 \
  --ctx-size 8192 \
  --n-predict 1024 \
  --temp 0.7 \
  --top-k 40 \
  --top-p 0.9 \
  --batch-size 512 \
  --repeat-penalty 1.1 \
  --system-prompt "你是一个智能助手，能够提供准确、有用的信息" \
  --interactive
</code></pre>