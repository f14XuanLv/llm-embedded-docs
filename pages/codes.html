<h1>源代码示例</h1>

<p>本页面展示了在 Gemma 3 ONNX 模型转换与推理过程中使用的核心 Python 脚本源代码。</p>

<h2>builder_simplified.py</h2>
<p>此脚本借鉴自 <code>microsoft/onnxruntime-genai</code> 项目的 <code>builder.py</code>，经过提取和简化，用于将 Gemma 3 1B 指令微调模型 (基于其特定的 <code>config.json</code>) 转换为 ONNX 格式。它显式构建 ONNX 计算图，处理 Gemma 3 的特有结构，并生成推理所需的 <code>genai_config.json</code>。</p>
<pre class="long-code-block"><code class="language-python">
# builder_simplified.py
"""
Run this script to create the desired ONNX model.
"""
from onnx import helper, numpy_helper, TensorProto, external_data_helper, save_model
from onnxruntime import __version__ as ort_version
from packaging import version
if version.parse(ort_version) > version.parse('1.21.1'):
    from onnxruntime.quantization.matmul_nbits_quantizer import MatMulNBitsQuantizer, QuantFormat
else:
    from onnxruntime.quantization.matmul_4bits_quantizer import MatMul4BitsQuantizer as MatMulNBitsQuantizer, QuantFormat
from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig
import numpy as np
import torch
import argparse
import gc
import json
import os
import textwrap

class Model:

    def __init__(self, config, io_dtype, onnx_dtype, ep, cache_dir, extra_options):
        self.context_length = config.seq_length if hasattr(config, 'seq_length') else config.max_position_embeddings
        self.original_context_length = config.original_max_position_embeddings if hasattr(config, 'original_max_position_embeddings') else config.rope_scaling['original_max_position_embeddings'] if hasattr(config, 'rope_scaling') and hasattr(config.rope_scaling, 'original_max_position_embeddings') else self.context_length
        self.window_size = config.sliding_window if hasattr(config, 'sliding_window') else -1
        self.intermediate_size = config.ffn_hidden_size if hasattr(config, 'ffn_hidden_size') else config.intermediate_size
        self.hidden_size = config.hidden_size
        self.num_kv_heads = config.num_key_value_heads if hasattr(config, 'num_key_value_heads') else config.multi_query_group_num if hasattr(config, 'multi_query_group_num') else config.num_attention_heads
        self.num_attn_heads = config.num_attention_heads
        self.head_size = config.head_dim if hasattr(config, 'head_dim') else config.hidden_size // config.num_attention_heads
        self.num_layers = int(extra_options['num_hidden_layers']) if 'num_hidden_layers' in extra_options else config.num_hidden_layers if hasattr(config, 'num_hidden_layers') else config.num_layers
        self.vocab_size = config.vocab_size
        self.activation = config.hidden_activation if hasattr(config, 'hidden_activation') and config.hidden_activation is not None else config.hidden_act
        self.model_name_or_path = config._name_or_path
        self.model_type = config.architectures[0]
        self.io_dtype = io_dtype
        self.onnx_dtype = onnx_dtype
        self.quant_type = config.quantization_config['quant_method'] if hasattr(config, 'quantization_config') else None
        self.adapter_path = extra_options.get('adapter_path', None)
        self.cache_dir = cache_dir
        self.filename = extra_options.get('filename', 'model.onnx')
        self.hf_token = parse_hf_token(extra_options.get('hf_token', 'true'))
        self.extra_options = extra_options
        self.inputs = []
        self.outputs = []
        self.initializers = []
        self.value_infos = []
        self.nodes = []
        self.ep = ep
        self.ep_attrs = {'cpu': {}, 'cuda': {'enable_cuda_graph': '1' if extra_options.get('enable_cuda_graph', False) else '0'}, 'rocm': {'tunable_op_enable': '1', 'tunable_op_tuning_enable': '1'}, 'dml': {}, 'webgpu': {}}
        self.input_names = ['input_ids', 'attention_mask', 'position_ids']
        self.input_types = {'input_ids': TensorProto.INT64, 'attention_mask': TensorProto.INT64, 'position_ids': TensorProto.INT64, 'inputs_embeds': self.io_dtype, 'past_key_values.key': self.io_dtype, 'past_key_values.value': self.io_dtype}
        self.input_shapes = {'input_ids': ['batch_size', 'sequence_length'], 'attention_mask': ['batch_size', 'total_sequence_length'], 'position_ids': ['batch_size', 'sequence_length'], 'inputs_embeds': ['batch_size', 'sequence_length', self.hidden_size], 'past_key_values.key': ['batch_size', self.num_kv_heads, 'past_sequence_length', self.head_size], 'past_key_values.value': ['batch_size', self.num_kv_heads, 'past_sequence_length', self.head_size]}
        self.exclude_embeds = extra_options.get('exclude_embeds', False)
        if self.exclude_embeds:
            self.input_names = [name.replace('input_ids', 'inputs_embeds') for name in self.input_names]
        self.output_names = ['logits']
        self.output_types = {'hidden_states': self.io_dtype, 'logits': self.io_dtype, 'present.key': self.io_dtype, 'present.value': self.io_dtype}
        self.output_shapes = {'hidden_states': ['batch_size', 'sequence_length', self.hidden_size], 'logits': ['batch_size', 'sequence_length', self.vocab_size], 'present.key': ['batch_size', self.num_kv_heads, 'total_sequence_length', self.head_size], 'present.value': ['batch_size', self.num_kv_heads, 'total_sequence_length', self.head_size]}
        self.exclude_lm_head = extra_options.get('exclude_lm_head', False)
        self.include_hidden_states = extra_options.get('include_hidden_states', False)
        if self.exclude_lm_head:
            self.output_names = [name.replace('logits', 'hidden_states') for name in self.output_names]
        elif self.include_hidden_states:
            self.output_names = ['hidden_states'] + self.output_names
        self.node_names = set()
        self.to_numpy_dtype = {TensorProto.INT8: np.uint8, TensorProto.INT32: np.int32, TensorProto.INT64: np.int64, TensorProto.FLOAT16: np.float16, TensorProto.FLOAT: np.float32}
        self.to_str_dtype = {TensorProto.INT8: 'TensorProto.INT8', TensorProto.INT32: 'TensorProto.INT32', TensorProto.INT64: 'TensorProto.INT64', TensorProto.FLOAT16: 'TensorProto.FLOAT16', TensorProto.FLOAT: 'TensorProto.FLOAT'}
        self.mask_attrs = {'mask_name': '', 'seqlens_k': '', 'total_seq_len': '', 'block_row_indices': '', 'block_col_indices': '', 'key_total_seq_lens': ''}
        self.embed_attrs = {'scale': 1}
        epsilon = config.rms_norm_eps if hasattr(config, 'rms_norm_eps') else 1e-06
        self.layernorm_attrs = {'simple': True, 'first_layernorm': True, 'last_layernorm': False, 'root_input': '', 'skip_input': '', 'output_0': '', 'output_3': '', 'add_offset': 0, 'epsilon': epsilon}
        is_lora = hasattr(config, 'peft_type') and config.peft_type == 'LORA'
        self.matmul_attrs = {'use_lora': is_lora}
        position_scale = config.rope_position_scale if hasattr(config, 'rope_position_scale') else 1
        partial_rotary_factor = config.partial_rotary_factor if hasattr(config, 'partial_rotary_factor') else 1.0
        rotemb_dim = int(self.head_size * partial_rotary_factor) if partial_rotary_factor != 1.0 else 0
        rope_theta = config.rope_theta if hasattr(config, 'rope_theta') else config.rope_embedding_base if hasattr(config, 'rope_embedding_base') else 10000
        self.rotemb_attrs = {'create_caches': True, 'save_caches': True, 'cache_length': self.context_length, 'theta': rope_theta, 'partial_rotary_factor': partial_rotary_factor, 'interleaved': 0, 'rotary_embedding_dim': rotemb_dim, 'rescale_factors': 1, 't_dtype': torch.int64, 'position_scale': position_scale, 'mscale': 1, 'mscale_policy': ''}
        if hasattr(config, 'rope_scaling') and config.rope_scaling is not None:
            if 'short_factor' in config.rope_scaling:
                self.rotemb_attrs['mscale_policy'] = config.rope_scaling['type']
                short_factor = torch.tensor(config.rope_scaling['short_factor'], dtype=torch.float32)
                long_factor = torch.tensor(config.rope_scaling['long_factor'], dtype=torch.float32)
                short_mscale = config.rope_scaling['short_mscale'] if 'short_mscale' in config.rope_scaling else 0
                long_mscale = config.rope_scaling['long_mscale'] if 'long_mscale' in config.rope_scaling else 0
                short_mscale = short_mscale if short_mscale > 0 else self.make_mscale(self.context_length / self.original_context_length)
                long_mscale = long_mscale if long_mscale > 0 else self.make_mscale(self.context_length / self.original_context_length)
                self.rotemb_attrs['multi_cache'] = {'short_factor': short_factor, 'long_factor': long_factor, 'short_mscale': short_mscale, 'long_mscale': long_mscale}
            elif 'low_freq_factor' in config.rope_scaling:
                factor = config.rope_scaling['factor'] if 'factor' in config.rope_scaling else 0
                low_freq_factor = config.rope_scaling['low_freq_factor'] if 'low_freq_factor' in config.rope_scaling else 0
                high_freq_factor = config.rope_scaling['high_freq_factor'] if 'high_freq_factor' in config.rope_scaling else 0
                self.rotemb_attrs['rescale_inv_freq'] = {'factor': factor, 'low_freq_factor': low_freq_factor, 'high_freq_factor': high_freq_factor}
        attn_softcap = config.attn_logit_softcapping if hasattr(config, 'attn_logit_softcapping') and config.attn_logit_softcapping is not None else 0.0
        sparse_block_size = config.blocksparse_block_size if hasattr(config, 'blocksparse_block_size') else 0
        kernel_block_size = config.blocksparse_triton_kernel_block_size if hasattr(config, 'blocksparse_triton_kernel_block_size') else 0
        local_blocks = config.blocksparse_num_local_blocks if hasattr(config, 'blocksparse_num_local_blocks') else 0
        vert_block_stride = config.blocksparse_vert_stride if hasattr(config, 'blocksparse_vert_stride') else 0
        homo_head = config.blocksparse_homo_head_pattern if hasattr(config, 'blocksparse_homo_head_pattern') else False
        self.attention_attrs = {'q_path': '', 'k_path': '', 'v_path': '', 'op_type': 'MultiHeadAttention', 'scale': 1 / np.sqrt(self.head_size), 'softcap': attn_softcap, 'use_rope_in_attn': False, 'use_packed_matmul': False, 'block_sparse': {'sparse_block_size': sparse_block_size, 'kernel_block_size': kernel_block_size, 'local_blocks': local_blocks, 'vert_stride': vert_block_stride, 'homo_head': homo_head}, 'q_norm': False, 'k_norm': False}
        self.make_attention_init()
        self.mlp_attrs = {'use_proj': True, 'use_fc': False, 'output_0': ''}
        num_experts = config.num_experts if hasattr(config, 'num_experts') else 1
        self.moe_attrs = {'num_experts': num_experts, 'top_k': 1, 'activation_type': 'relu', 'normalize_routing_weights': False, 'use_sparse_mixer': False, 'use_int4': True}
        lm_head_softcap = config.final_logit_softcapping if hasattr(config, 'final_logit_softcapping') and config.final_logit_softcapping is not None else 0.0
        self.lm_head_attrs = {'scale': 1, 'mask': None, 'softcap': lm_head_softcap}
        if hasattr(config, 'dummy_token_indices'):
            dummy_tokens_mask = torch.zeros(self.vocab_size).bool()
            dummy_tokens_mask[config.dummy_token_indices] = True
            self.lm_head_attrs['mask'] = dummy_tokens_mask
        self.quant_attrs = {'int4': {'accuracy_level': int(extra_options.get('int4_accuracy_level', 4 if self.ep == 'cpu' else 0)), 'block_size': int(extra_options.get('int4_block_size', 32)), 'is_symmetric': extra_options.get('int4_is_symmetric', True), 'op_types_to_quantize': extra_options.get('int4_op_types_to_quantize', ('MatMul',)), 'nodes_to_exclude': extra_options.get('int4_nodes_to_exclude', [])}, 'use_qdq': extra_options.get('use_qdq', False)}
        if self.quant_type is not None:
            self.quant_attrs['config'] = config.quantization_config
            self.quant_attrs['use_g_idx'] = config.quantization_config['desc_act'] if 'desc_act' in config.quantization_config else False

    def make_attention_init(self):
        valid_gqa_configurations = [('cpu', TensorProto.FLOAT), ('cuda', TensorProto.FLOAT16), ('rocm', TensorProto.FLOAT16), ('dml', TensorProto.FLOAT16), ('webgpu', TensorProto.FLOAT16), ('webgpu', TensorProto.FLOAT)]
        if (self.ep, self.io_dtype) in valid_gqa_configurations:
            self.attention_attrs['op_type'] = 'GroupQueryAttention'
            print('GroupQueryAttention (GQA) is used in this model.')
            self.attention_attrs['use_packed_matmul'] = self.ep not in ['dml', 'webgpu'] and (not self.matmul_attrs['use_lora']) and (not self.attention_attrs['q_norm']) and (not self.attention_attrs['k_norm'])
            self.attention_attrs['use_rope_in_attn'] = self.ep not in ['dml', 'webgpu'] and (not self.attention_attrs['q_norm']) and (not self.attention_attrs['k_norm'])
            if self.attention_attrs['use_rope_in_attn']:
                self.input_names.remove('position_ids')
        self.past_present_share_buffer = self.attention_attrs['op_type'] == 'GroupQueryAttention'

    def make_genai_config(self, model_name_or_path, extra_kwargs, out_dir):
        try:
            config = GenerationConfig.from_pretrained(model_name_or_path, token=self.hf_token, trust_remote_code=True, **extra_kwargs)
        except:
            config = AutoConfig.from_pretrained(model_name_or_path, token=self.hf_token, trust_remote_code=True, **extra_kwargs)
        inputs = dict(zip(self.input_names, self.input_names))
        inputs.update({'past_key_names': 'past_key_values.%d.key', 'past_value_names': 'past_key_values.%d.value'})
        outputs = dict(zip(self.output_names, self.output_names))
        outputs.update({'present_key_names': 'present.%d.key', 'present_value_names': 'present.%d.value'})
        if 'hidden_states' in outputs:
            del outputs['hidden_states']
        genai_config = {'model': {'bos_token_id': config.bos_token_id if hasattr(config, 'bos_token_id') and config.bos_token_id != None else 1, 'context_length': self.context_length, 'decoder': {'session_options': {'log_id': 'onnxruntime-genai', 'provider_options': []}, 'filename': self.filename, 'head_size': self.head_size, 'hidden_size': self.hidden_size, 'inputs': inputs, 'outputs': outputs, 'num_attention_heads': self.num_attn_heads, 'num_hidden_layers': self.num_layers, 'num_key_value_heads': self.num_kv_heads}, 'eos_token_id': config.eos_token_id, 'pad_token_id': config.pad_token_id if hasattr(config, 'pad_token_id') and config.pad_token_id is not None else config.eos_token_id[0] if isinstance(config.eos_token_id, list) else config.eos_token_id, 'type': self.model_type[:self.model_type.find('For') if 'For' in self.model_type else len(self.model_type)].lower(), 'vocab_size': self.vocab_size}, 'search': {'diversity_penalty': config.diversity_penalty if hasattr(config, 'diversity_penalty') else 0.0, 'do_sample': config.do_sample if hasattr(config, 'do_sample') else False, 'early_stopping': True, 'length_penalty': config.length_penalty if hasattr(config, 'length_penalty') else 1.0, 'max_length': self.context_length, 'min_length': 0, 'no_repeat_ngram_size': config.no_repeat_ngram_size if hasattr(config, 'no_repeat_ngram_size') else 0, 'num_beams': config.num_beams if hasattr(config, 'num_beams') else 1, 'num_return_sequences': config.num_return_sequences if hasattr(config, 'num_return_sequences') else 1, 'past_present_share_buffer': False if 'config_only' in self.extra_options else self.past_present_share_buffer, 'repetition_penalty': config.repetition_penalty if hasattr(config, 'repetition_penalty') else 1.0, 'temperature': config.temperature if hasattr(config, 'temperature') else 1.0, 'top_k': 1, 'top_p': config.top_p if hasattr(config, 'top_p') else 1.0}}
        if self.ep != 'cpu':
            ep_options = {self.ep: self.ep_attrs[self.ep]}
            genai_config['model']['decoder']['session_options']['provider_options'].append(ep_options)
        if self.extra_options.get('include_prompt_templates', False):
            prompt_templates = self._get_prompt_templates(model_name_or_path, extra_kwargs)
            if prompt_templates is not None:
                genai_config['model']['prompt_templates'] = prompt_templates
        print(f'Saving GenAI config in {out_dir}')
        with open(os.path.join(out_dir, 'genai_config.json'), 'w') as f:
            json.dump(genai_config, f, indent=4)

    def save_processing(self, model_name_or_path, extra_kwargs, out_dir):
        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, token=self.hf_token, trust_remote_code=True, **extra_kwargs)
        print(f'Saving processing files in {out_dir} for GenAI')
        tokenizer.save_pretrained(out_dir)

    def _get_prompt_templates(self, hf_name, extra_kwargs):
        try:
            tokenizer = AutoTokenizer.from_pretrained(hf_name, token=self.hf_token, trust_remote_code=True, eos_token=None, **extra_kwargs)
            system_template = tokenizer.apply_chat_template([{'role': 'system', 'content': '{Content}'}], tokenize=False)
            system_user_template = tokenizer.apply_chat_template([{'role': 'system', 'content': '{Content}'}, {'role': 'user', 'content': '{Content}'}], tokenize=False)
            system_user_assistant_template = tokenizer.apply_chat_template([{'role': 'system', 'content': '{Content}'}, {'role': 'user', 'content': '{Content}'}, {'role': 'assistant', 'content': '{Content}'}], tokenize=False)
            assert system_user_template.startswith(system_template), 'Chat templates may contain padding tokens, leading to incorrect prompt templates'
            assert system_user_assistant_template.startswith(system_user_template), 'Chat templates may contain padding tokens, leading to incorrect prompt templates'
            user_template = system_user_template[len(system_template):]
            assistant_template = system_user_assistant_template[len(system_user_template):]
            prompt_template = system_user_assistant_template[len(system_template):]
            prompt_template = prompt_template[:prompt_template.rfind('{Content}')]
            templates = {'system': system_template, 'user': user_template, 'assistant': assistant_template, 'prompt': prompt_template}
            return templates
        except Exception as e:
            print(f'Failed to get prompt templates. Error: {e}')
            return None

    def save_model(self, out_dir):
        print(f'Saving ONNX model in {out_dir}')
        gc.collect()
        model = helper.make_model(opset_imports=[self.clear_field(helper.make_operatorsetid('', 21 if self.quant_attrs['use_qdq'] else 14), 'domain'), helper.make_operatorsetid('com.microsoft', 1)], ir_version=7, producer_name='onnxruntime-genai', producer_version='0.0.0', graph=self.make_graph(name='main_graph', inputs=self.inputs, outputs=self.outputs, initializer=self.initializers, value_info=self.value_infos, nodes=self.nodes))
        external_data_helper.load_external_data_for_model(model, self.cache_dir)
        for path in os.listdir(self.cache_dir):
            if path.endswith('.bin'):
                os.remove(os.path.join(self.cache_dir, path))
        if len(os.listdir(self.cache_dir)) == 0:
            os.rmdir(self.cache_dir)
        already_quantized_in_qdq_format = self.quant_type is not None and self.quant_attrs['use_qdq']
        if self.onnx_dtype == 'int4' and (not already_quantized_in_qdq_format):
            model = self.to_int4(model)
        out_path = os.path.join(out_dir, self.filename)
        data_path = os.path.join(out_dir, os.path.basename(out_path) + '.data')
        if os.path.exists(out_path):
            print(f'Overwriting {out_path}')
            os.remove(out_path)
        if os.path.exists(data_path):
            print(f'Overwriting {data_path}')
            os.remove(data_path)
        save_model(model, out_path, save_as_external_data=True, all_tensors_to_one_file=True, location=os.path.basename(data_path), size_threshold=0, convert_attribute=False)

    def to_int4(self, model):
        quant = MatMulNBitsQuantizer(model=model, block_size=self.quant_attrs['int4']['block_size'], is_symmetric=self.quant_attrs['int4']['is_symmetric'], accuracy_level=self.quant_attrs['int4']['accuracy_level'], nodes_to_exclude=self.quant_attrs['int4']['nodes_to_exclude'], quant_format=QuantFormat.QDQ if self.quant_attrs['use_qdq'] else QuantFormat.QOperator, op_types_to_quantize=self.quant_attrs['int4']['op_types_to_quantize'])
        quant.process()
        return quant.model.model

    def clear_field(self, proto, field):
        proto.ClearField(field)
        return proto

    def order_repeated_field(self, repeated_proto, key_name, order):
        order = list(order)
        repeated_proto.sort(key=lambda x: order.index(getattr(x, key_name)))

    def make_external_tensor(self, np_data, name, unpack_int4=False, **kwargs):
        tensor = numpy_helper.from_array(np_data)
        tensor.name = name
        filename = f'{name}.bin'
        external_data_helper.set_external_data(tensor, location=filename)
        with open(os.path.join(self.cache_dir, filename), 'wb') as f:
            f.write(tensor.raw_data)
        tensor.ClearField('raw_data')
        tensor.data_location = TensorProto.EXTERNAL
        if unpack_int4 and self.onnx_dtype == 'int4':
            tensor.data_type = TensorProto.UINT4
            tensor.dims[-1] *= 2
        self.initializers.append(tensor)

    def make_node(self, op_type, inputs, outputs, name=None, doc_string=None, domain=None, **kwargs):
        for input_name in inputs:
            if input_name.startswith('/model/constants') and input_name not in self.node_names:
                self.make_constant(input_name)
        if name not in self.node_names:
            node = helper.make_node(op_type, inputs, outputs, name, doc_string, domain, **kwargs)
            if doc_string == '':
                node.doc_string = ''
            self.order_repeated_field(node.attribute, 'name', kwargs.keys())
            self.nodes.append(node)
            self.node_names.add(name)

    def make_value_info(self, name, dtype, shape):
        value_info = helper.make_tensor_value_info(name, dtype, shape=shape)
        self.value_infos.append(value_info)

    def make_graph(self, *args, doc_string=None, **kwargs):
        graph = helper.make_graph(*args, doc_string=doc_string, **kwargs)
        if doc_string == '':
            graph.doc_string = ''
        return graph

    def make_inputs_and_outputs(self):
        inputs = []
        for name in self.input_names:
            dtype = self.input_types[name]
            shape = self.input_shapes[name]
            inputs.append(helper.make_tensor_value_info(name, dtype, shape=shape))
        outputs = []
        for name in self.output_names:
            dtype = self.output_types[name]
            shape = self.output_shapes[name]
            outputs.append(helper.make_tensor_value_info(name, dtype, shape=shape))
        for i in range(self.num_layers):
            key_name = f'past_key_values.{i}.key'
            inputs.append(helper.make_tensor_value_info(key_name, self.input_types['past_key_values.key'], shape=self.input_shapes['past_key_values.key']))
            value_name = f'past_key_values.{i}.value'
            inputs.append(helper.make_tensor_value_info(value_name, self.input_types['past_key_values.value'], shape=self.input_shapes['past_key_values.value']))
            key_name = f'present.{i}.key'
            outputs.append(helper.make_tensor_value_info(key_name, self.output_types['present.key'], shape=self.output_shapes['present.key']))
            value_name = f'present.{i}.value'
            outputs.append(helper.make_tensor_value_info(value_name, self.output_types['present.value'], shape=self.output_shapes['present.value']))
        self.inputs = inputs
        self.outputs = outputs

    def make_constant(self, name):
        path = name.split('/')
        (onnx_dtype, dims, num) = (eval(path[-3]), path[-2], eval(path[-1]))
        np_dtype = self.to_numpy_dtype[onnx_dtype]
        value = numpy_helper.from_array(np.array(num if dims == '0D' else list(num) if type(num) == tuple else [num], dtype=np_dtype), name=name.replace('constants', 'numpy_helper'))
        node_name = name.replace('constants', 'constant_nodes')
        self.make_node('Constant', inputs=[], outputs=[name], name=node_name, value=value)
        self.make_value_info(name, onnx_dtype, shape=[])
        self.node_names.add(name)

    def make_gather(self, name, inputs, axis):
        output = f'{name}/output_0'
        self.make_node('Gather', inputs=inputs, outputs=[output], name=name, axis=axis)
        self.make_value_info(output, TensorProto.INT64, shape=[])

    def make_reshape(self, name, inputs, dtype, shape):
        output = f'{name}/output_0'
        self.make_node('Reshape', inputs=inputs, outputs=[output], name=name)
        self.make_value_info(output, dtype, shape=shape)

    def make_shape(self, name, root_input, shape):
        output = f'{name}/output_0'
        self.make_node('Shape', inputs=[root_input], outputs=[output], name=name)
        self.make_value_info(output, TensorProto.INT64, shape=shape)

    def make_constant_of_shape(self, name, root_input, value, dtype, shape):
        output = f'{name}/output_0'
        self.make_node('ConstantOfShape', inputs=[root_input], outputs=[output], name=name, value=value)
        self.make_value_info(output, dtype, shape=shape)

    def make_unsqueeze(self, name, inputs, dtype, shape):
        output = f'{name}/output_0'
        self.make_node('Unsqueeze', inputs=inputs, outputs=[output], name=name)
        self.make_value_info(output, dtype, shape=shape)

    def make_squeeze(self, name, inputs):
        output = f'{name}/output_0'
        self.make_node('Squeeze', inputs=inputs, outputs=[output], name=name)
        self.make_value_info(output, TensorProto.INT64, shape=[])

    def make_concat(self, name, inputs, dtype, shape, axis=0):
        output = f'{name}/output_0'
        self.make_node('Concat', inputs=inputs, outputs=[output], name=name, axis=axis)
        self.make_value_info(output, dtype, shape=shape)

    def make_tile(self, name, inputs, dtype, shape):
        output = f'{name}/output_0'
        self.make_node('Tile', inputs=inputs, outputs=[output], name=name)
        self.make_value_info(output, dtype, shape=shape)

    def make_equal(self, name, inputs, shape):
        output = f'{name}/output_0'
        self.make_node('Equal', inputs=inputs, outputs=[output], name=name)
        self.make_value_info(output, TensorProto.BOOL, shape=shape)

    def make_greater(self, name, inputs, shape):
        output = f'{name}/output_0'
        self.make_node('Greater', inputs=inputs, outputs=[output], name=name)
        self.make_value_info(output, TensorProto.BOOL, shape=shape)

    def make_greater_or_equal(self, name, inputs, shape):
        output = f'{name}/output_0'
        self.make_node('GreaterOrEqual', inputs=inputs, outputs=[output], name=name)
        self.make_value_info(output, TensorProto.BOOL, shape=shape)

    def make_isinf(self, name, root_input, shape):
        output = f'{name}/output_0'
        self.make_node('IsInf', inputs=[root_input], outputs=[output], name=name)
        self.make_value_info(output, TensorProto.BOOL, shape=shape)

    def make_clip(self, name, inputs, dtype, shape):
        output = f'{name}/output_0'
        self.make_node('Clip', inputs=inputs, outputs=[output], name=name)
        self.make_value_info(output, dtype, shape=shape)

    def make_where(self, name, inputs, dtype, shape):
        output = f'{name}/output_0'
        self.make_node('Where', inputs=inputs, outputs=[output], name=name)
        self.make_value_info(output, dtype, shape=shape)

    def make_expand(self, name, inputs, dtype, shape):
        output = f'{name}/output_0'
        self.make_node('Expand', inputs=inputs, outputs=[output], name=name)
        self.make_value_info(output, dtype, shape=shape)

    def make_reduce_sum(self, name, inputs, dtype, shape):
        output = f'{name}/output_0'
        self.make_node('ReduceSum', inputs=inputs, outputs=[output], name=name)
        self.make_value_info(output, dtype, shape=shape)

    def make_reduce_max(self, name, inputs, dtype, shape):
        output = f'{name}/output_0'
        self.make_node('ReduceMax', inputs=inputs, outputs=[output], name=name, keepdims=False)
        self.make_value_info(output, dtype, shape=shape)

    def make_cast(self, name, root_input, dtype, shape):
        output = f'{name}/output_0'
        self.make_node('Cast', inputs=[root_input], outputs=[output], name=name, to=dtype)
        self.make_value_info(output, dtype, shape=shape)

    def make_add(self, name, inputs, dtype, shape):
        output = f'{name}/output_0'
        self.make_node('Add', inputs=inputs, outputs=[output], name=name)
        self.make_value_info(output, dtype, shape=shape)

    def make_sub(self, name, inputs, dtype, shape):
        output = f'{name}/output_0'
        self.make_node('Sub', inputs=inputs, outputs=[output], name=name)
        self.make_value_info(output, dtype, shape=shape)

    def make_less(self, name, inputs):
        output = f'{name}/output_0'
        self.make_node('Less', inputs=inputs, outputs=[output], name=name)
        self.make_value_info(output, TensorProto.BOOL, shape=None)

    def make_range(self, name, inputs):
        output = f'{name}/output_0'
        self.make_node('Range', inputs=inputs, outputs=[output], name=name)
        self.make_value_info(output, TensorProto.INT64, shape=['unk'])

    def make_slice(self, name, inputs, dtype, shape):
        output = f'{name}/output_0'
        self.make_node('Slice', inputs=inputs, outputs=[output], name=name)
        self.make_value_info(output, dtype, shape=shape)

    def make_mul(self, name, inputs, dtype, shape):
        output = f'{name}/output_0'
        self.make_node('Mul', inputs=inputs, outputs=[output], name=name)
        self.make_value_info(output, dtype, shape=shape)

    def make_transpose(self, name, root_input, dtype, shape, perm):
        output = f'{name}/output_0'
        self.make_node('Transpose', inputs=[root_input], outputs=[output], name=name, perm=perm)
        self.make_value_info(output, dtype, shape=shape)

    def make_div(self, name, inputs, dtype, shape):
        output = f'{name}/output_0'
        self.make_node('Div', inputs=inputs, outputs=[output], name=name)
        self.make_value_info(output, dtype, shape=shape)

    def make_tanh(self, name, root_input, dtype, shape):
        output = f'{name}/output_0'
        self.make_node('Tanh', inputs=[root_input], outputs=[output], name=name)
        self.make_value_info(output, dtype, shape=shape)

    def make_matmul(self, matmul, basename, root_input, **kwargs):
        if hasattr(matmul, 'base_layer'):
            return self.make_matmul_lora(matmul, basename, root_input, **kwargs)
        else:
            return self.make_matmul_op(matmul, basename, root_input, **kwargs)

    def make_matmul_op(self, matmul, basename, root_input, **kwargs):
        if self.onnx_dtype in {'fp16', 'fp32'}:
            return self.make_matmul_fp16_or_fp32(matmul, basename, root_input, **kwargs)
        elif self.onnx_dtype == 'int4':
            if self.quant_attrs['use_qdq']:
                return self.make_matmul_int4_qdq(matmul, basename, root_input, **kwargs)
            else:
                return self.make_matmul_int4(matmul, basename, root_input, **kwargs)
        else:
            raise NotImplementedError(f'The {self.onnx_dtype} precision is not currently supported.')

    def make_matmul_fp16_or_fp32(self, matmul, name, root_input, **kwargs):
        weight = name[1:].replace('/', '.') + '.weight'
        self.make_external_tensor(matmul.weight.detach().numpy().transpose().astype(self.to_numpy_dtype[self.io_dtype]), weight)
        last_dim = matmul.weight.shape[0]
        output = 'logits' if kwargs.get('logits', False) else f'{name}/output_0'
        self.make_node('MatMul', inputs=[root_input, weight], outputs=[output], name=name)
        self.make_value_info(output, self.io_dtype, shape=['batch_size', 'sequence_length', last_dim])
        return name

    def make_matmul_int4(self, matmul, basename, root_input, **kwargs):
        if not hasattr(matmul, 'qweight'):
            return self.make_matmul_fp16_or_fp32(matmul, basename, root_input, **kwargs)
        name = f'{basename}NBits'
        weight = name[1:].replace('/', '.') + '.qweight'
        self.make_external_tensor(matmul.qweight.detach().numpy(), weight)
        scales = name[1:].replace('/', '.') + '.scales'
        self.make_external_tensor(matmul.scales.detach().numpy().astype(self.to_numpy_dtype[self.io_dtype]), scales)
        inputs = [root_input, weight, scales]
        if hasattr(matmul, 'qzeros') and matmul.qzeros is not None:
            zeros = name[1:].replace('/', '.') + '.qzeros'
            self.make_external_tensor(matmul.qzeros.detach().numpy(), zeros)
            inputs.append(zeros)
        if hasattr(matmul, 'g_idx') and matmul.g_idx is not None:
            g_idx = name[1:].replace('/', '.') + '.g_idx'
            self.make_external_tensor(matmul.g_idx.detach().numpy().astype(np.int32), g_idx)
            inputs.append(g_idx)
        output = 'logits' if kwargs.get('logits', False) else f'{name}/output_0'
        self.make_node('MatMulNBits', inputs=inputs, outputs=[output], name=name, domain='com.microsoft', accuracy_level=self.quant_attrs['int4']['accuracy_level'], bits=matmul.bits, block_size=matmul.group_size, K=matmul.in_features, N=matmul.out_features)
        self.make_value_info(output, self.io_dtype, shape=['batch_size', 'sequence_length', matmul.out_features])
        return name

    def make_dequantize_linear(self, dequantize_name, quantized_op):
        qweight = dequantize_name[1:].replace('/', '.') + '.qweight'
        qweight_npy = quantized_op.qweight.detach().numpy()
        qweight_npy = qweight_npy.reshape(*qweight_npy.shape[:-2], qweight_npy.shape[-2] * qweight_npy.shape[-1])
        self.make_external_tensor(qweight_npy, qweight, True)
        scales = dequantize_name[1:].replace('/', '.') + '.scales'
        scales_npy = quantized_op.scales.detach().numpy().astype(self.to_numpy_dtype[self.io_dtype])
        scales_npy = scales_npy.reshape(*qweight_npy.shape[:-1], qweight_npy.shape[-1] * 2 // quantized_op.group_size)
        self.make_external_tensor(scales_npy, scales)
        dequantize_inputs = [qweight, scales]
        if hasattr(quantized_op, 'qzeros') and quantized_op.qzeros is not None:
            zeros = dequantize_name[1:].replace('/', '.') + '.qzeros'
            zeros_npy = quantized_op.qzeros.detach().numpy()
            zeros_npy = zeros_npy.reshape(*qweight_npy.shape[:-1], qweight_npy.shape[-1] // quantized_op.group_size)
            self.make_external_tensor(zeros_npy, zeros, True)
            dequantize_inputs.append(zeros)
        dequantize_output = f'{dequantize_name}/output_0'
        self.make_node('DequantizeLinear', inputs=dequantize_inputs, outputs=[dequantize_output], name=dequantize_name, block_size=quantized_op.group_size, axis=-1)
        self.make_value_info(dequantize_output, self.io_dtype, shape=[*scales_npy.shape[:-1], scales_npy.shape[-1] * quantized_op.group_size])
        return dequantize_output

    def make_matmul_int4_qdq(self, matmul, matmul_name, root_input, **kwargs):
        if not hasattr(matmul, 'qweight'):
            return self.make_matmul_fp16_or_fp32(matmul, matmul_name, root_input, **kwargs)
        dequantize_output = self.make_dequantize_linear(f'{matmul_name}/DequantizeLinear', matmul)
        qweight_shape = matmul.qweight.detach().numpy().shape
        transposed_shape = [qweight_shape[1] * qweight_shape[2] * 2, qweight_shape[0]]
        transpose_name = f'{matmul_name}/Transpose'
        self.make_transpose(transpose_name, dequantize_output, self.io_dtype, transposed_shape, [1, 0])
        matmul_output = 'logits' if kwargs.get('logits', False) else f'{matmul_name}/output_0'
        self.make_node('MatMul', inputs=[root_input, f'{transpose_name}/output_0'], outputs=[matmul_output], name=matmul_name)
        self.make_value_info(matmul_output, self.io_dtype, shape=['batch_size', 'sequence_length', matmul.out_features])
        return matmul_name

    def make_matmul_lora(self, matmul, basename, root_input, **kwargs):
        basename_parts = basename.split('/')
        matmul_A_basename = '/'.join(basename_parts[:-1] + ['lora_A'] + basename_parts[-1:])
        matmul_A_name = self.make_matmul_op(matmul.lora_A.default, matmul_A_basename, root_input=root_input)
        lora_A = f'{matmul_A_name}/output_0'
        matmul.lora_B.default.weight.requires_grad = False
        matmul.lora_B.default.weight *= matmul.scaling['default']
        matmul_B_basename = '/'.join(basename_parts[:-1] + ['lora_B'] + basename_parts[-1:])
        matmul_B_name = self.make_matmul_op(matmul.lora_B.default, matmul_B_basename, root_input=lora_A)
        lora_B = f'{matmul_B_name}/output_0'
        last_dim = matmul.base_layer.weight.shape[0]
        matmul_name = self.make_matmul_op(matmul.base_layer, basename, root_input, **kwargs)
        add_name = '/'.join(basename_parts[:-1] + ['lora', 'Add'])
        add_inputs = [f'{matmul_name}/output_0', lora_B]
        add_shape = ['batch_size', 'sequence_length', last_dim]
        self.make_add(add_name, add_inputs, dtype=self.io_dtype, shape=add_shape)
        return add_name

    def make_packed_matmul(self, q_matmul, k_matmul, v_matmul, basename, root_input, **kwargs):
        if self.onnx_dtype in {'fp16', 'fp32'}:
            return self.make_packed_matmul_fp16_or_fp32(q_matmul, k_matmul, v_matmul, basename, root_input, **kwargs)
        elif self.onnx_dtype == 'int4':
            return self.make_packed_matmul_int4(q_matmul, k_matmul, v_matmul, basename, root_input, **kwargs)
        else:
            raise NotImplementedError(f'The {self.onnx_dtype} precision is not currently supported.')

    def make_packed_matmul_fp16_or_fp32(self, q_matmul, k_matmul, v_matmul, name, root_input, **kwargs):
        (N_q, H) = q_matmul.weight.shape
        (N_kv, _) = k_matmul.weight.shape

        class PackedMatMul:

            def __init__(self):
                self.weight = torch.concatenate([q_matmul.weight.detach().cpu(), k_matmul.weight.detach().cpu(), v_matmul.weight.detach().cpu()], dim=0).reshape(N_q + N_kv + N_kv, H)
        matmul = PackedMatMul()
        new_name = self.make_matmul(matmul, name, root_input, **kwargs)
        return new_name

    def make_packed_matmul_int4(self, q_matmul, k_matmul, v_matmul, basename, root_input, **kwargs):
        if not hasattr(q_matmul, 'qweight'):
            return self.make_packed_matmul_fp16_or_fp32(q_matmul, k_matmul, v_matmul, basename, root_input, **kwargs)
        name = f'{basename}NBits'

        class PackedMatMul:

            def __init__(self):
                self.qweight = torch.concatenate([q_matmul.qweight.detach().cpu(), k_matmul.qweight.detach().cpu(), v_matmul.qweight.detach().cpu()], dim=0)
                self.scales = torch.concatenate([q_matmul.scales.detach().cpu(), k_matmul.scales.detach().cpu(), v_matmul.scales.detach().cpu()], dim=0)
                self.qzeros = torch.concatenate([q_matmul.qzeros.detach().cpu(), k_matmul.qzeros.detach().cpu(), v_matmul.qzeros.detach().cpu()], dim=0)
                self.g_idx = q_matmul.g_idx
                self.in_features = q_matmul.in_features
                self.out_features = q_matmul.out_features + k_matmul.out_features + v_matmul.out_features
                self.bits = q_matmul.bits
                self.group_size = q_matmul.group_size
        matmul = PackedMatMul()
        weight = name[1:].replace('/', '.') + '.qweight'
        self.make_external_tensor(matmul.qweight.detach().numpy(), weight)
        scales = name[1:].replace('/', '.') + '.scales'
        self.make_external_tensor(matmul.scales.detach().numpy().astype(self.to_numpy_dtype[self.io_dtype]), scales)
        inputs = [root_input, weight, scales]
        if hasattr(matmul, 'qzeros') and matmul.qzeros is not None:
            zeros = name[1:].replace('/', '.') + '.qzeros'
            self.make_external_tensor(matmul.qzeros.detach().numpy(), zeros)
            inputs.append(zeros)
        if hasattr(matmul, 'g_idx') and matmul.g_idx is not None:
            g_idx = name[1:].replace('/', '.') + '.g_idx'
            self.make_external_tensor(matmul.g_idx.detach().numpy().astype(np.int32), g_idx)
            inputs.append(g_idx)
        output = 'logits' if kwargs.get('logits', False) else f'{name}/output_0'
        self.make_node('MatMulNBits', inputs=inputs, outputs=[output], name=name, domain='com.microsoft', accuracy_level=self.quant_attrs['int4']['accuracy_level'], bits=matmul.bits, block_size=matmul.group_size, K=matmul.in_features, N=matmul.out_features)
        self.make_value_info(output, self.io_dtype, shape=['batch_size', 'sequence_length', matmul.out_features])
        return name

    def make_add_bias(self, add, name, root_input, **kwargs):
        bias = name[1:].replace('/', '.') + '.bias'
        self.make_external_tensor(add.astype(self.to_numpy_dtype[self.io_dtype]), bias)
        add_bias_inputs = [root_input, bias]
        shape = ['batch_size', 'sequence_length', add.shape[0]]
        if 'logits' in kwargs:
            output = 'logits'
            self.make_node('Add', inputs=add_bias_inputs, outputs=[output], name=name)
            self.make_value_info(output, dtype=self.io_dtype, shape=shape)
        else:
            self.make_add(name, add_bias_inputs, dtype=self.io_dtype, shape=shape)

    def make_packed_add(self, q_add, k_add, v_add, name, root_input, **kwargs):
        add = np.concatenate([q_add, k_add, v_add], axis=0).flatten()
        self.make_add_bias(add, name, root_input, **kwargs)

    def make_embedding(self, embedding):
        weight = 'model.embed_tokens.weight'
        self.make_external_tensor(embedding.astype(self.to_numpy_dtype[self.io_dtype]), weight)
        basename = '/model/embed_tokens'
        gather_name = f'{basename}/Gather'
        gather_output = f'{gather_name}/output_0'
        self.make_node('Gather', inputs=[weight, 'input_ids'], outputs=[gather_output], name=gather_name)
        self.make_value_info(gather_output, self.io_dtype, shape=['batch_size', 'sequence_length', self.hidden_size])
        if self.embed_attrs['scale'] != 1:
            mul_name = f'{basename}/Mul'
            mul_inputs = [gather_output, f"/model/constants/{self.to_str_dtype[self.io_dtype]}/0D/{self.embed_attrs['scale']}"]
            mul_output = f'{mul_name}/output_0'
            self.make_node('Mul', inputs=mul_inputs, outputs=[mul_output], name=mul_name)
            self.make_value_info(mul_output, self.io_dtype, shape=['batch_size', 'sequence_length', self.hidden_size])
            layernorm_attrs_value = mul_output
        else:
            layernorm_attrs_value = gather_output
        self.layernorm_attrs['root_input'] = layernorm_attrs_value
        self.layernorm_attrs['skip_input'] = layernorm_attrs_value

    def make_layernorm(self, layer_id, layernorm, skip, simple, location):
        root_input = self.layernorm_attrs['root_input']
        skip_input = self.layernorm_attrs['skip_input']
        weight = f'model.layers.{layer_id}.{location}_layernorm.weight'
        self.make_external_tensor(layernorm.weight.detach().numpy().astype(self.to_numpy_dtype[self.io_dtype]) + self.layernorm_attrs['add_offset'], weight)
        bias = f'model.layers.{layer_id}.{location}_layernorm.bias'
        if not simple:
            self.make_external_tensor(layernorm.bias.detach().numpy().astype(self.to_numpy_dtype[self.io_dtype]), bias)
        inputs = [root_input, skip_input, weight] if skip else [root_input, weight]
        if not simple:
            inputs.append(bias)
        name = f"/model/layers.{layer_id}/{location}_layernorm/{('Skip' if skip else '')}LayerNorm"
        op_type = f"{('Skip' if skip else '')}{('Simplified' if simple else '')}LayerNormalization"
        kwargs = {'epsilon': self.layernorm_attrs['epsilon']}
        if not skip:
            kwargs.update({'axis': -1, 'stash_type': 1})
        output_0 = f'/model/layers.{layer_id}/{location}_layernorm/output_0'
        output_3 = f'/model/layers.{layer_id}/{location}_layernorm/output_3'
        if self.layernorm_attrs['last_layernorm'] and (self.include_hidden_states or self.exclude_lm_head):
            output_0 = 'hidden_states'
        outputs = [output_0, '', '', output_3] if skip and (not self.layernorm_attrs['last_layernorm']) else [output_0]
        self.make_node(op_type, inputs=inputs, outputs=outputs, name=name, domain='com.microsoft' if skip else None, **kwargs)
        self.make_value_info(output_0, self.io_dtype, shape=['batch_size', 'sequence_length', self.hidden_size])
        if skip and (not self.layernorm_attrs['last_layernorm']):
            self.make_value_info(output_3, self.io_dtype, shape=['batch_size', 'sequence_length', self.hidden_size])
        self.layernorm_attrs['output_0'] = output_0
        if skip and (not self.layernorm_attrs['last_layernorm']):
            self.layernorm_attrs['output_3'] = output_3
            self.layernorm_attrs['root_input'] = output_3

    def make_mscale_su(self, mscale):
        if mscale <= 1.0:
            return 1.0
        return np.sqrt(1 + np.log(mscale) / np.log(self.original_context_length))

    def make_mscale_yarn(self, mscale):
        if mscale <= 1.0:
            return 1.0
        return 0.1 * np.log(mscale) + 1.0

    def make_mscale(self, mscale):
        if self.rotemb_attrs['mscale_policy'] in {'su', 'longrope'}:
            return self.make_mscale_su(mscale)
        elif self.rotemb_attrs['mscale_policy'] == 'yarn':
            return self.make_mscale_yarn(mscale)
        else:
            return float(mscale)

    def make_inv_freq_rescaled(self, inv_freq):
        scale_factor = self.rotemb_attrs['rescale_inv_freq']['factor']
        low_freq_factor = self.rotemb_attrs['rescale_inv_freq']['low_freq_factor']
        high_freq_factor = self.rotemb_attrs['rescale_inv_freq']['high_freq_factor']
        old_context_len = self.original_context_length
        low_freq_wavelen = old_context_len / low_freq_factor
        high_freq_wavelen = old_context_len / high_freq_factor
        new_freqs = []
        for freq in inv_freq:
            wavelen = 2 * torch.pi / freq
            if wavelen < high_freq_wavelen:
                new_freqs.append(freq)
            elif wavelen > low_freq_wavelen:
                new_freqs.append(freq / scale_factor)
            else:
                smooth = (old_context_len / wavelen - low_freq_factor) / (high_freq_factor - low_freq_factor)
                new_freqs.append((1 - smooth) * freq / scale_factor + smooth * freq)
        return torch.tensor(new_freqs, dtype=inv_freq.dtype)

    def make_rotary_embedding_caches_from_scratch(self):
        dim = int(self.rotemb_attrs['partial_rotary_factor'] * self.head_size)
        inv_freq = 1.0 / (self.rotemb_attrs['rescale_factors'] * self.rotemb_attrs['theta'] ** (torch.arange(0, dim, 2, dtype=torch.int64).float() / dim))
        if 'rescale_inv_freq' in self.rotemb_attrs:
            inv_freq = self.make_inv_freq_rescaled(inv_freq)
        position_scale = self.rotemb_attrs['position_scale'] if self.context_length == self.original_context_length else 1
        t = (torch.arange(self.rotemb_attrs['cache_length'], dtype=self.rotemb_attrs['t_dtype']) * position_scale).type_as(inv_freq)
        freqs = torch.outer(t, inv_freq)
        emb = torch.cat((freqs, freqs), dim=-1)
        (cos_cache, sin_cache) = (emb.cos() * self.rotemb_attrs['mscale'], emb.sin() * self.rotemb_attrs['mscale'])
        return (cos_cache, sin_cache)

    def make_rotary_embedding_caches(self, **kwargs):
        cos_cache_name = kwargs.get('cos_cache_name', 'cos_cache')
        sin_cache_name = kwargs.get('sin_cache_name', 'sin_cache')
        if self.rotemb_attrs['create_caches']:
            (cos_cache, sin_cache) = self.make_rotary_embedding_caches_from_scratch()
            hidden_dim = cos_cache.shape[-1]
            cos_cache = cos_cache.squeeze()[:, :hidden_dim // 2].detach().numpy()
            cos_cache = cos_cache.astype(self.to_numpy_dtype[self.io_dtype])
            sin_cache = sin_cache.squeeze()[:, :hidden_dim // 2].detach().numpy()
            sin_cache = sin_cache.astype(self.to_numpy_dtype[self.io_dtype])
            if self.rotemb_attrs['partial_rotary_factor'] != 1.0:
                cos_cache = cos_cache[:, :self.rotemb_attrs['rotary_embedding_dim'] // 2]
                sin_cache = sin_cache[:, :self.rotemb_attrs['rotary_embedding_dim'] // 2]
            if self.rotemb_attrs['save_caches']:
                self.make_external_tensor(cos_cache, cos_cache_name)
                self.make_external_tensor(sin_cache, sin_cache_name)
            else:
                return (cos_cache, sin_cache)
            self.rotemb_attrs['create_caches'] = False
        return (cos_cache_name, sin_cache_name)

    def make_rotary_embedding(self, name, root_input, **kwargs):
        (cos_cache_name, sin_cache_name) = self.make_rotary_embedding_caches()
        num_heads = self.num_kv_heads if 'k_rotary' in name else self.num_attn_heads
        inputs = [root_input, kwargs.pop('position_ids'), cos_cache_name, sin_cache_name]
        output = f'{name}/output_0'
        self.make_node('RotaryEmbedding', inputs=inputs, outputs=[output], name=name, domain='com.microsoft', interleaved=self.rotemb_attrs['interleaved'], num_heads=0 if self.rotemb_attrs['partial_rotary_factor'] == 1.0 else num_heads, rotary_embedding_dim=self.rotemb_attrs['rotary_embedding_dim'])
        self.make_value_info(output, self.io_dtype, shape=['batch_size', 'sequence_length', self.head_size * num_heads])

    def make_rotary_embedding_multi_cache(self):
        (if_cos_cache_output, if_sin_cache_output) = ('cos_cache', 'sin_cache')
        self.rotemb_attrs['rescale_factors'] = self.rotemb_attrs['multi_cache']['long_factor']
        self.rotemb_attrs['cache_length'] = self.context_length
        self.rotemb_attrs['mscale'] = self.rotemb_attrs['multi_cache']['long_mscale']
        if self.ep == 'dml':
            self.make_rotary_embedding_caches()
            self.make_value_info(if_cos_cache_output, self.io_dtype, shape=['max_sequence_length', 'head_dim / 2'])
            self.make_value_info(if_sin_cache_output, self.io_dtype, shape=['max_sequence_length', 'head_dim / 2'])
            return
        self.rotemb_attrs['save_caches'] = False
        (cos_cache_large_name, sin_cache_large_name) = ('cos_cache_large', 'sin_cache_large')
        (cos_cache_large, sin_cache_large) = self.make_rotary_embedding_caches(cos_cache_name=cos_cache_large_name, sin_cache_name=sin_cache_large_name)
        self.rotemb_attrs['rescale_factors'] = self.rotemb_attrs['multi_cache']['short_factor']
        self.rotemb_attrs['cache_length'] = self.original_context_length
        self.rotemb_attrs['mscale'] = self.rotemb_attrs['multi_cache']['short_mscale']
        (cos_cache_small_name, sin_cache_small_name) = ('cos_cache_small', 'sin_cache_small')
        (cos_cache_small, sin_cache_small) = self.make_rotary_embedding_caches(cos_cache_name=cos_cache_small_name, sin_cache_name=sin_cache_small_name)
        self.rotemb_attrs['create_caches'] = False
        basename = '/model/rotemb_caches_subgraph'
        gather_name = ''
        if self.attention_attrs['op_type'] == 'GroupQueryAttention':
            gather_name = '/model/attn_mask_reformat/attn_mask_subgraph/Gather'
        else:
            gather_name = '/model/attn_mask_reformat/attn_mask_subgraph/Gather_2'
        greater_name = f'{basename}/Greater'
        greater_inputs = [f'{gather_name}/output_0', f'/model/constants/TensorProto.INT64/0D/{self.original_context_length}']
        self.make_greater(greater_name, greater_inputs, shape=[])
        if_name = f'{basename}/If'
        self.make_node('If', inputs=[f'{greater_name}/output_0'], outputs=[if_cos_cache_output, if_sin_cache_output], name=if_name, then_branch=self.make_graph(name='large_rotemb_caches_graph', inputs=[], outputs=[helper.make_tensor_value_info(cos_cache_large_name, self.io_dtype, shape=cos_cache_large.shape), helper.make_tensor_value_info(sin_cache_large_name, self.io_dtype, shape=sin_cache_large.shape)], initializer=[], value_info=[], nodes=[helper.make_node('Constant', inputs=[], outputs=[cos_cache_large_name], name='/large/cos_cache/Constant', value=numpy_helper.from_array(cos_cache_large)), helper.make_node('Constant', inputs=[], outputs=[sin_cache_large_name], name='/large/sin_cache/Constant', value=numpy_helper.from_array(sin_cache_large))]), else_branch=self.make_graph(name='small_rotemb_caches_graph', inputs=[], outputs=[helper.make_tensor_value_info(cos_cache_small_name, self.io_dtype, shape=cos_cache_small.shape), helper.make_tensor_value_info(sin_cache_small_name, self.io_dtype, shape=sin_cache_small.shape)], initializer=[], value_info=[], nodes=[helper.make_node('Constant', inputs=[], outputs=[cos_cache_small_name], name='/small/cos_cache/Constant', value=numpy_helper.from_array(cos_cache_small)), helper.make_node('Constant', inputs=[], outputs=[sin_cache_small_name], name='/small/sin_cache/Constant', value=numpy_helper.from_array(sin_cache_small))]))
        self.make_value_info(if_cos_cache_output, self.io_dtype, shape=['max_sequence_length', 'head_dim / 2'])
        self.make_value_info(if_sin_cache_output, self.io_dtype, shape=['max_sequence_length', 'head_dim / 2'])

    def make_qk_norm(self, layer_id, attention):
        layernorm_kwargs = {'epsilon': self.layernorm_attrs['epsilon'], 'axis': -1, 'stash_type': 1}
        q_reshape_1_name = f'/model/layers.{layer_id}/attn/q_norm/Reshape_1'
        q_reshape_1_inputs = [self.attention_attrs['q_path'], f'/model/constants/TensorProto.INT64/1D/0, -1, {self.head_size}']
        q_reshape_1_output = f'{q_reshape_1_name}/output_0'
        self.make_reshape(q_reshape_1_name, q_reshape_1_inputs, dtype=self.io_dtype, shape=['batch_size', 'sequence_length * num_attention_heads', self.head_size])
        q_layernorm_name = f'/model/layers.{layer_id}/attn/q_norm/SimplifiedLayerNormalization'
        q_weight_name = f'model.layers.{layer_id}.attn.q_norm.layernorm.weight'
        q_layernorm_output = f'{q_layernorm_name}/output_0'
        self.make_external_tensor(attention.q_norm.weight.detach().numpy().astype(self.to_numpy_dtype[self.io_dtype]) + self.layernorm_attrs['add_offset'], q_weight_name)
        self.make_node('SimplifiedLayerNormalization', inputs=[q_reshape_1_output, q_weight_name], outputs=[q_layernorm_output], name=q_layernorm_name, **layernorm_kwargs)
        self.make_value_info(q_layernorm_output, dtype=self.io_dtype, shape=['batch_size', 'sequence_length * num_attention_heads', self.head_size])
        q_reshape_2_name = f'/model/layers.{layer_id}/attn/q_norm/Reshape_2'
        q_reshape_2_inputs = [q_layernorm_output, f'/model/constants/TensorProto.INT64/1D/0, -1, {self.num_attn_heads * self.head_size}']
        self.make_reshape(q_reshape_2_name, q_reshape_2_inputs, dtype=self.io_dtype, shape=['batch_size', 'sequence_length', self.num_attn_heads * self.head_size])
        k_reshape_1_name = f'/model/layers.{layer_id}/attn/k_norm/Reshape_1'
        k_reshape_1_inputs = [self.attention_attrs['k_path'], f'/model/constants/TensorProto.INT64/1D/0, -1, {self.head_size}']
        k_reshape_1_output = f'{k_reshape_1_name}/output_0'
        self.make_reshape(k_reshape_1_name, k_reshape_1_inputs, dtype=self.io_dtype, shape=['batch_size', 'sequence_length * num_key_value_heads', self.head_size])
        k_layernorm_name = f'/model/layers.{layer_id}/attn/k_norm/SimplifiedLayerNormalization'
        k_weight_name = f'model.layers.{layer_id}.attn.k_norm.layernorm.weight'
        k_layernorm_output = f'{k_layernorm_name}/output_0'
        self.make_external_tensor(attention.k_norm.weight.detach().numpy().astype(self.to_numpy_dtype[self.io_dtype]) + self.layernorm_attrs['add_offset'], k_weight_name)
        self.make_node('SimplifiedLayerNormalization', inputs=[k_reshape_1_output, k_weight_name], outputs=[k_layernorm_output], name=k_layernorm_name, **layernorm_kwargs)
        self.make_value_info(k_layernorm_output, dtype=self.io_dtype, shape=['batch_size', 'sequence_length * num_key_value_heads', self.head_size])
        k_reshape_2_name = f'/model/layers.{layer_id}/attn/k_norm/Reshape_2'
        k_reshape_2_inputs = [k_layernorm_output, f'/model/constants/TensorProto.INT64/1D/0, -1, {self.num_kv_heads * self.head_size}']
        self.make_reshape(k_reshape_2_name, k_reshape_2_inputs, dtype=self.io_dtype, shape=['batch_size', 'sequence_length', self.num_kv_heads * self.head_size])
        self.attention_attrs['q_path'] = f'{q_reshape_2_name}/output_0'
        self.attention_attrs['k_path'] = f'{k_reshape_2_name}/output_0'

    def make_repeat_kv(self, layer_id, root_input, past_kv, present_kv, **kwargs):
        basename = f"/model/layers.{layer_id}/attn/{('k_proj' if past_kv.endswith('key') else 'v_proj')}/repeat_kv"
        reshape_1_name = f'{basename}/Reshape_1'
        reshape_1_inputs = [root_input, f'/model/constants/TensorProto.INT64/1D/0, 0, {self.num_kv_heads}, -1']
        self.make_reshape(reshape_1_name, reshape_1_inputs, dtype=self.io_dtype, shape=['batch_size', 'sequence_length', self.num_kv_heads, self.head_size])
        transpose_1_name = f'{basename}/Transpose_1'
        transpose_1_input = f'{reshape_1_name}/output_0'
        self.make_transpose(transpose_1_name, transpose_1_input, dtype=self.io_dtype, shape=['batch_size', self.num_kv_heads, 'sequence_length', self.head_size], perm=[0, 2, 1, 3])
        concat_1_name = f'{basename}/Concat_1'
        concat_1_inputs = [past_kv, f'{transpose_1_name}/output_0']
        self.make_node('Concat', inputs=concat_1_inputs, outputs=[present_kv], name=concat_1_name, axis=2)
        shape_1_name = f'{basename}/Shape_1'
        self.make_shape(shape_1_name, present_kv, shape=[4])
        gather_1_name = f'{basename}/Gather_1'
        gather_1_inputs = [f'{shape_1_name}/output_0', '/model/constants/TensorProto.INT64/0D/0']
        self.make_gather(gather_1_name, gather_1_inputs, axis=0)
        unsqueeze_1_name = f'{basename}/Unsqueeze_1'
        unsqueeze_1_inputs = [f'{gather_1_name}/output_0', '/model/constants/TensorProto.INT64/1D/0']
        self.make_unsqueeze(unsqueeze_1_name, unsqueeze_1_inputs, dtype=TensorProto.INT64, shape=[1])
        gather_2_name = f'{basename}/Gather_2'
        gather_2_inputs = [f'{shape_1_name}/output_0', '/model/constants/TensorProto.INT64/0D/1']
        self.make_gather(gather_2_name, gather_2_inputs, axis=0)
        unsqueeze_2_name = f'{basename}/Unsqueeze_2'
        unsqueeze_2_inputs = [f'{gather_2_name}/output_0', '/model/constants/TensorProto.INT64/1D/0']
        self.make_unsqueeze(unsqueeze_2_name, unsqueeze_2_inputs, dtype=TensorProto.INT64, shape=[1])
        gather_3_name = f'{basename}/Gather_3'
        gather_3_inputs = [f'{shape_1_name}/output_0', '/model/constants/TensorProto.INT64/0D/2']
        self.make_gather(gather_3_name, gather_3_inputs, axis=0)
        unsqueeze_3_name = f'{basename}/Unsqueeze_3'
        unsqueeze_3_inputs = [f'{gather_3_name}/output_0', '/model/constants/TensorProto.INT64/1D/0']
        self.make_unsqueeze(unsqueeze_3_name, unsqueeze_3_inputs, dtype=TensorProto.INT64, shape=[1])
        gather_4_name = f'{basename}/Gather_4'
        gather_4_inputs = [f'{shape_1_name}/output_0', '/model/constants/TensorProto.INT64/0D/3']
        self.make_gather(gather_4_name, gather_4_inputs, axis=0)
        unsqueeze_4_name = f'{basename}/Unsqueeze_4'
        unsqueeze_4_inputs = [f'{gather_4_name}/output_0', '/model/constants/TensorProto.INT64/1D/0']
        self.make_unsqueeze(unsqueeze_4_name, unsqueeze_4_inputs, dtype=TensorProto.INT64, shape=[1])
        concat_2_name = f'{basename}/Concat_2'
        concat_2_inputs = [f'{unsqueeze_1_name}/output_0', f'{unsqueeze_2_name}/output_0', f'/model/constants/TensorProto.INT64/1D/{self.num_attn_heads // self.num_kv_heads}', f'{unsqueeze_3_name}/output_0', f'{unsqueeze_4_name}/output_0']
        self.make_concat(concat_2_name, concat_2_inputs, dtype=TensorProto.INT64, shape=[5], axis=0)
        mul_1_name = f'{basename}/Mul_1'
        mul_1_inputs = [f'{unsqueeze_2_name}/output_0', f'/model/constants/TensorProto.INT64/0D/{self.num_attn_heads // self.num_kv_heads}']
        self.make_mul(mul_1_name, mul_1_inputs, dtype=TensorProto.INT64, shape=None)
        concat_3_name = f'{basename}/Concat_3'
        concat_3_inputs = [f'{unsqueeze_1_name}/output_0', f'{mul_1_name}/output_0', f'{unsqueeze_3_name}/output_0', f'{unsqueeze_4_name}/output_0']
        self.make_concat(concat_3_name, concat_3_inputs, dtype=TensorProto.INT64, shape=[4], axis=0)
        reshape_2_name = f'{basename}/Reshape_2'
        reshape_2_inputs = [f'{concat_2_name}/output_0', '/model/constants/TensorProto.INT64/1D/-1']
        self.make_reshape(reshape_2_name, reshape_2_inputs, dtype=TensorProto.INT64, shape=None)
        shape_2_name = f'{basename}/Shape_2'
        self.make_shape(shape_2_name, f'{reshape_2_name}/output_0', shape=[1])
        constant_shape_name = f'{basename}/ConstantOfShape'
        constant_shape_value = numpy_helper.from_array(np.array([1], dtype='int64'))
        self.make_constant_of_shape(constant_shape_name, f'{shape_2_name}/output_0', value=constant_shape_value, dtype=TensorProto.INT64, shape=[5])
        mul_2_name = f'{basename}/Mul'
        mul_2_inputs = [f'{constant_shape_name}/output_0', '/model/constants/TensorProto.INT64/0D/-1']
        self.make_mul(mul_2_name, mul_2_inputs, dtype=TensorProto.INT64, shape=[5])
        equal_name = f'{basename}/Equal'
        equal_inputs = [f'{reshape_2_name}/output_0', f'{mul_2_name}/output_0']
        self.make_equal(equal_name, equal_inputs, shape=[5])
        where_name = f'{basename}/Where'
        where_inputs = [f'{equal_name}/output_0', f'{constant_shape_name}/output_0', f'{reshape_2_name}/output_0']
        self.make_where(where_name, where_inputs, dtype=TensorProto.INT64, shape=[5])
        unsqueeze_5_name = f'{basename}/Unsqueeze_5'
        unsqueeze_5_inputs = [present_kv, '/model/constants/TensorProto.INT64/1D/2']
        self.make_unsqueeze(unsqueeze_5_name, unsqueeze_5_inputs, dtype=self.io_dtype, shape=['batch_size', self.num_kv_heads, 1, 'sequence_length', self.head_size])
        expand_name = f'{basename}/Expand'
        expand_inputs = [f'{unsqueeze_5_name}/output_0', f'{where_name}/output_0']
        self.make_expand(expand_name, expand_inputs, dtype=self.io_dtype, shape=['batch_size', self.num_kv_heads, self.num_attn_heads // self.num_kv_heads, 'sequence_length', self.head_size])
        reshape_3_name = f'{basename}/Reshape_3'
        reshape_3_inputs = [f'{expand_name}/output_0', f'{concat_3_name}/output_0']
        self.make_reshape(reshape_3_name, reshape_3_inputs, dtype=self.io_dtype, shape=['batch_size', self.num_attn_heads, 'sequence_length', self.head_size])
        transpose_2_name = f'{basename}/Transpose_2'
        transpose_2_input = f'{reshape_3_name}/output_0'
        self.make_transpose(transpose_2_name, transpose_2_input, dtype=self.io_dtype, shape=['batch_size', 'sequence_length', self.num_attn_heads, self.head_size], perm=[0, 2, 1, 3])
        reshape_4_name = f'{basename}/Reshape_4'
        reshape_4_inputs = [f'{transpose_2_name}/output_0', f'/model/constants/TensorProto.INT64/1D/0, 0, {self.num_attn_heads * self.head_size}']
        self.make_reshape(reshape_4_name, reshape_4_inputs, dtype=self.io_dtype, shape=['batch_size', 'sequence_length', self.num_attn_heads * self.head_size])
        input_to_attention = f'{reshape_4_name}/output_0'
        return input_to_attention

    def make_attention_op(self, name, **kwargs):
        op_type = self.attention_attrs['op_type']
        if op_type == 'MultiHeadAttention':
            self.make_multi_head_attention(name, add_qk=f"{self.mask_attrs['mask_name']}/output_0", **kwargs)
        elif op_type == 'GroupQueryAttention':
            self.make_group_query_attention(name, seqlens_k=f"{self.mask_attrs['seqlens_k']}/output_0", total_seq_len=f"{self.mask_attrs['total_seq_len']}/output_0", **kwargs)
        elif op_type == 'SparseAttention':
            self.make_sparse_attention(name, block_row_indices=self.mask_attrs['block_row_indices'], block_col_indices=self.mask_attrs['block_col_indices'], key_total_seq_lens=f"{self.mask_attrs['key_total_seq_lens']}/output_0", total_seq_len=f"{self.mask_attrs['total_seq_len']}/output_0", **kwargs)
        else:
            raise NotImplementedError(f'The {op_type} op is not currently supported.')

    def make_multi_head_attention(self, name, **kwargs):
        inputs = [kwargs['q_path'], kwargs['k_path'], kwargs['v_path'], kwargs.get('bias', ''), kwargs.get('attn_mask', ''), kwargs.get('add_qk', ''), kwargs.get('past_k', ''), kwargs.get('past_v', '')]
        output = f'{name}/output_0'
        outputs = [output, kwargs.get('present_k', ''), kwargs.get('present_v', '')]
        self.make_node('MultiHeadAttention', inputs=inputs, outputs=outputs, name=name, domain='com.microsoft', num_heads=self.num_attn_heads, scale=self.attention_attrs['scale'])
        self.make_value_info(output, self.io_dtype, shape=['batch_size', 'sequence_length', self.head_size * self.num_attn_heads])

    def make_group_query_attention(self, name, **kwargs):
        inputs = [kwargs['q_path'], kwargs['k_path'], kwargs['v_path'], kwargs.get('past_k', ''), kwargs.get('past_v', ''), kwargs.get('seqlens_k', ''), kwargs.get('total_seq_len', ''), kwargs.get('cos_cache', ''), kwargs.get('sin_cache', '')]
        output = f'{name}/output_0'
        outputs = [output, kwargs.get('present_k', ''), kwargs.get('present_v', '')]
        self.make_node('GroupQueryAttention', inputs=inputs, outputs=outputs, name=name, domain='com.microsoft', num_heads=self.num_attn_heads, kv_num_heads=self.num_kv_heads, scale=self.attention_attrs['scale'], local_window_size=self.window_size, softcap=self.attention_attrs['softcap'], do_rotary=self.attention_attrs['use_rope_in_attn'], rotary_interleaved=self.rotemb_attrs['interleaved'])
        self.make_value_info(output, self.io_dtype, shape=['batch_size', 'sequence_length', self.head_size * self.num_attn_heads])

    def make_sparse_attention(self, name, **kwargs):
        inputs = [kwargs['q_path'], kwargs['k_path'], kwargs['v_path'], kwargs.get('past_k'), kwargs.get('past_v'), kwargs.get('block_row_indices'), kwargs.get('block_col_indices'), kwargs.get('total_seq_len'), kwargs.get('key_total_seq_lens'), kwargs.get('cos_cache', ''), kwargs.get('sin_cache', '')]
        output = f'{name}/output_0'
        outputs = [output, kwargs.get('present_k', ''), kwargs.get('present_v', '')]
        self.make_node('SparseAttention', inputs=inputs, outputs=outputs, name=name, domain='com.microsoft', num_heads=self.num_attn_heads, kv_num_heads=self.num_kv_heads, scale=self.attention_attrs['scale'], sparse_block_size=self.attention_attrs['block_sparse']['sparse_block_size'], do_rotary=self.attention_attrs['use_rope_in_attn'], rotary_interleaved=self.rotemb_attrs['interleaved'])

    def make_attention(self, layer_id, attention, root_input, **kwargs):
        self.make_attention_unpacked(layer_id, attention, root_input, **kwargs)
        if self.attention_attrs['use_packed_matmul']:
            qkv_matmul_basename = f'/model/layers.{layer_id}/attn/qkv_proj/MatMul'
            qkv_matmul_name = self.make_packed_matmul(attention.q_proj, attention.k_proj, attention.v_proj, qkv_matmul_basename, root_input)
            self.attention_attrs['q_path'] = f'{qkv_matmul_name}/output_0'
        else:
            q_matmul_basename = f'/model/layers.{layer_id}/attn/q_proj/MatMul'
            q_matmul_name = self.make_matmul(attention.q_proj, q_matmul_basename, root_input)
            self.attention_attrs['q_path'] = f'{q_matmul_name}/output_0'
            k_matmul_basename = f'/model/layers.{layer_id}/attn/k_proj/MatMul'
            k_matmul_name = self.make_matmul(attention.k_proj, k_matmul_basename, root_input)
            self.attention_attrs['k_path'] = f'{k_matmul_name}/output_0'
            v_matmul_basename = f'/model/layers.{layer_id}/attn/v_proj/MatMul'
            v_matmul_name = self.make_matmul(attention.v_proj, v_matmul_basename, root_input)
            self.attention_attrs['v_path'] = f'{v_matmul_name}/output_0'
        q_bias_exists = attention.q_proj.bias is not None and torch.count_nonzero(attention.q_proj.bias) > 0
        k_bias_exists = attention.k_proj.bias is not None and torch.count_nonzero(attention.k_proj.bias) > 0
        v_bias_exists = attention.v_proj.bias is not None and torch.count_nonzero(attention.v_proj.bias) > 0
        all_bias_exists = q_bias_exists and k_bias_exists and v_bias_exists
        if all_bias_exists and self.attention_attrs['use_packed_matmul']:
            qkv_add_name = f'/model/layers.{layer_id}/attn/qkv_proj/Add'
            self.make_packed_add(attention.q_proj.bias.detach().numpy(), attention.k_proj.bias.detach().numpy(), attention.v_proj.bias.detach().numpy(), qkv_add_name, root_input=self.attention_attrs['q_path'])
            self.attention_attrs['q_path'] = f'{qkv_add_name}/output_0'
        else:
            if q_bias_exists:
                q_add_name = f'/model/layers.{layer_id}/attn/q_proj/Add'
                self.make_add_bias(attention.q_proj.bias.detach().numpy(), q_add_name, root_input=self.attention_attrs['q_path'])
                self.attention_attrs['q_path'] = f'{q_add_name}/output_0'
            if k_bias_exists:
                k_add_name = f'/model/layers.{layer_id}/attn/k_proj/Add'
                self.make_add_bias(attention.k_proj.bias.detach().numpy(), k_add_name, root_input=self.attention_attrs['k_path'])
                self.attention_attrs['k_path'] = f'{k_add_name}/output_0'
            if v_bias_exists:
                v_add_name = f'/model/layers.{layer_id}/attn/v_proj/Add'
                self.make_add_bias(attention.v_proj.bias.detach().numpy(), v_add_name, root_input=self.attention_attrs['v_path'])
                self.attention_attrs['v_path'] = f'{v_add_name}/output_0'
        if self.attention_attrs['q_norm'] and self.attention_attrs['k_norm']:
            self.make_qk_norm(layer_id, attention)
        (cos_cache_name, sin_cache_name) = ('', '')
        if self.attention_attrs['use_rope_in_attn']:
            (cos_cache_name, sin_cache_name) = self.make_rotary_embedding_caches()
        else:
            q_rotary_name = f'/model/layers.{layer_id}/attn/q_rotary/RotaryEmbedding'
            self.make_rotary_embedding(q_rotary_name, root_input=self.attention_attrs['q_path'], position_ids=kwargs.get('position_ids', 'position_ids'))
            self.attention_attrs['q_path'] = f'{q_rotary_name}/output_0'
            k_rotary_name = f'/model/layers.{layer_id}/attn/k_rotary/RotaryEmbedding'
            self.make_rotary_embedding(k_rotary_name, root_input=self.attention_attrs['k_path'], position_ids=kwargs.get('position_ids', 'position_ids'))
            self.attention_attrs['k_path'] = f'{k_rotary_name}/output_0'
        past_k = f'past_key_values.{layer_id}.key'
        past_v = f'past_key_values.{layer_id}.value'
        present_k = f'present.{layer_id}.key'
        present_v = f'present.{layer_id}.value'
        if self.num_attn_heads != self.num_kv_heads and self.attention_attrs['op_type'] == 'MultiHeadAttention':
            self.attention_attrs['k_path'] = self.make_repeat_kv(layer_id, root_input=self.attention_attrs['k_path'], past_kv=past_k, present_kv=present_k)
            self.attention_attrs['v_path'] = self.make_repeat_kv(layer_id, root_input=self.attention_attrs['v_path'], past_kv=past_v, present_kv=present_v)
            (past_k, past_v, present_k, present_v) = ('', '', '', '')
        attn_name = f"/model/layers.{layer_id}/attn/{self.attention_attrs['op_type']}"
        self.make_attention_op(attn_name, q_path=self.attention_attrs['q_path'], k_path=self.attention_attrs['k_path'], v_path=self.attention_attrs['v_path'], past_k=past_k, past_v=past_v, present_k=present_k, present_v=present_v, cos_cache=cos_cache_name, sin_cache=sin_cache_name, **kwargs)
        o_proj = 'o_proj' if hasattr(attention, 'o_proj') else 'dense'
        o_matmul_basename = f'/model/layers.{layer_id}/attn/o_proj/MatMul'
        o_weight = eval(f'attention.{o_proj}')
        o_matmul_name = self.make_matmul(o_weight, o_matmul_basename, f'{attn_name}/output_0')
        o_bias_exists = eval(f'attention.{o_proj}.bias') is not None
        if o_bias_exists:
            o_add_name = f'/model/layers.{layer_id}/attn/o_proj/Add'
            o_bias = eval(f'attention.{o_proj}.bias.detach().numpy()')
            self.make_add_bias(o_bias, o_add_name, root_input=f'{o_matmul_name}/output_0')
        self.layernorm_attrs['skip_input'] = f'{(o_matmul_name if not o_bias_exists else o_add_name)}/output_0'

    def make_attention_unpacked(self, layer_id, attention, root_input, **kwargs):
        qkv_linear = getattr(attention, 'qkv_proj', None) or getattr(attention, 'query_key_value', None)
        if qkv_linear is None:
            return
        if hasattr(qkv_linear, 'base_layer'):
            self.make_attention_unpacked_lora(layer_id, attention, qkv_linear, root_input, **kwargs)
        else:
            self.make_attention_unpacked_regular(layer_id, attention, qkv_linear, root_input, **kwargs)
        del qkv_linear

    def make_attention_unpacked_lora(self, layer_id, attention, qkv_linear, root_input, **kwargs):
        from peft.tuners.lora.layer import LoraLayer
        q_size = self.num_attn_heads * self.head_size
        kv_size = self.num_kv_heads * self.head_size
        q_proj = torch.nn.Linear(in_features=q_size, out_features=q_size)
        q_proj.weight = torch.nn.Parameter(qkv_linear.weight[:q_size, :], requires_grad=False)
        q_proj.bias = None if qkv_linear.bias is None else torch.nn.Parameter(qkv_linear.bias[:q_size], requires_grad=False)
        k_proj = torch.nn.Linear(in_features=q_size, out_features=kv_size)
        k_proj.weight = torch.nn.Parameter(qkv_linear.weight[q_size:q_size + kv_size, :], requires_grad=False)
        k_proj.bias = None if qkv_linear.bias is None else torch.nn.Parameter(qkv_linear.bias[q_size:q_size + kv_size], requires_grad=False)
        v_proj = torch.nn.Linear(in_features=q_size, out_features=kv_size)
        v_proj.weight = torch.nn.Parameter(qkv_linear.weight[q_size + kv_size:, :], requires_grad=False)
        v_proj.bias = None if qkv_linear.bias is None else torch.nn.Parameter(qkv_linear.bias[q_size + kv_size:], requires_grad=False)
        lora_B = qkv_linear.lora_B.default
        q_lora_B = torch.nn.Linear(in_features=q_size, out_features=q_size)
        q_lora_B.weight = torch.nn.Parameter(lora_B.weight[:q_size, :], requires_grad=False)
        q_lora_B.bias = None if lora_B.bias is None else torch.nn.Parameter(lora_B.bias[:q_size], requires_grad=False)
        k_lora_B = torch.nn.Linear(in_features=q_size, out_features=kv_size)
        k_lora_B.weight = torch.nn.Parameter(lora_B.weight[q_size:q_size + kv_size, :], requires_grad=False)
        k_lora_B.bias = None if lora_B.bias is None else torch.nn.Parameter(lora_B.bias[q_size:q_size + kv_size], requires_grad=False)
        v_lora_B = torch.nn.Linear(in_features=q_size, out_features=kv_size)
        v_lora_B.weight = torch.nn.Parameter(lora_B.weight[q_size + kv_size:, :], requires_grad=False)
        v_lora_B.bias = None if lora_B.bias is None else torch.nn.Parameter(lora_B.bias[q_size + kv_size:], requires_grad=False)
        attention.q_proj = LoraLayer(q_proj)
        attention.q_proj.lora_A.default = qkv_linear.lora_A.default
        attention.q_proj.lora_B.default = q_lora_B
        attention.q_proj.scaling = qkv_linear.scaling
        attention.k_proj = LoraLayer(k_proj)
        attention.k_proj.lora_A.default = qkv_linear.lora_A.default
        attention.k_proj.lora_B.default = k_lora_B
        attention.k_proj.scaling = qkv_linear.scaling
        attention.v_proj = LoraLayer(v_proj)
        attention.v_proj.lora_A.default = qkv_linear.lora_A.default
        attention.v_proj.lora_B.default = v_lora_B
        attention.v_proj.scaling = qkv_linear.scaling

    def make_attention_unpacked_regular(self, layer_id, attention, qkv_linear, root_input, **kwargs):
        q_size = self.num_attn_heads * self.head_size
        kv_size = self.num_kv_heads * self.head_size
        attention.q_proj = torch.nn.Linear(in_features=q_size, out_features=q_size)
        attention.q_proj.weight = torch.nn.Parameter(qkv_linear.weight[:q_size, :], requires_grad=False)
        attention.q_proj.bias = None if qkv_linear.bias is None else torch.nn.Parameter(qkv_linear.bias[:q_size], requires_grad=False)
        attention.k_proj = torch.nn.Linear(in_features=q_size, out_features=kv_size)
        attention.k_proj.weight = torch.nn.Parameter(qkv_linear.weight[q_size:q_size + kv_size, :], requires_grad=False)
        attention.k_proj.bias = None if qkv_linear.bias is None else torch.nn.Parameter(qkv_linear.bias[q_size:q_size + kv_size], requires_grad=False)
        attention.v_proj = torch.nn.Linear(in_features=q_size, out_features=kv_size)
        attention.v_proj.weight = torch.nn.Parameter(qkv_linear.weight[q_size + kv_size:, :], requires_grad=False)
        attention.v_proj.bias = None if qkv_linear.bias is None else torch.nn.Parameter(qkv_linear.bias[q_size + kv_size:], requires_grad=False)

    def make_mlp(self, layer_id, mlp, root_input):
        self.make_mlp_unpacked(layer_id, mlp, root_input)
        if self.mlp_attrs['use_proj']:
            self.make_mlp_proj(layer_id, mlp, root_input)
        elif self.mlp_attrs['use_fc']:
            self.make_mlp_fc(layer_id, mlp, root_input)
        else:
            raise NotImplementedError(f'The MLP layer type is not set.')

    def make_mlp_unpacked(self, layer_id, mlp, root_input):
        gate_up_linear = getattr(mlp, 'gate_up_proj', None) or getattr(mlp, 'dense_h_to_4h', None)
        if gate_up_linear is None:
            return
        if hasattr(gate_up_linear, 'base_layer'):
            self.make_mlp_unpacked_lora(layer_id, mlp, gate_up_linear, root_input)
        else:
            self.make_mlp_unpacked_regular(layer_id, mlp, gate_up_linear, root_input)
        del gate_up_linear

    def make_mlp_unpacked_lora(self, layer_id, mlp, gate_up_linear, root_input):
        from peft.tuners.lora.layer import LoraLayer
        gate_proj = torch.nn.Linear(in_features=self.hidden_size, out_features=self.intermediate_size)
        gate_proj.weight = torch.nn.Parameter(gate_up_linear.weight[:self.intermediate_size, :], requires_grad=False)
        gate_proj.bias = None if gate_up_linear.bias is None else torch.nn.Parameter(gate_up_linear.bias[:self.intermediate_size], requires_grad=False)
        up_proj = torch.nn.Linear(in_features=self.hidden_size, out_features=self.intermediate_size)
        up_proj.weight = torch.nn.Parameter(gate_up_linear.weight[self.intermediate_size:, :], requires_grad=False)
        up_proj.bias = None if gate_up_linear.bias is None else torch.nn.Parameter(gate_up_linear.bias[self.intermediate_size:], requires_grad=False)
        lora_B = gate_up_linear.lora_B.default
        gate_proj_lora_B = torch.nn.Linear(in_features=self.hidden_size, out_features=self.intermediate_size)
        gate_proj_lora_B.weight = torch.nn.Parameter(lora_B.weight[:self.intermediate_size, :], requires_grad=False)
        gate_proj_lora_B.bias = None if lora_B.bias is None else torch.nn.Parameter(lora_B.bias[:self.intermediate_size], requires_grad=False)
        up_proj_lora_B = torch.nn.Linear(in_features=self.hidden_size, out_features=self.intermediate_size)
        up_proj_lora_B.weight = torch.nn.Parameter(lora_B.weight[self.intermediate_size:, :], requires_grad=False)
        up_proj_lora_B.bias = None if lora_B.bias is None else torch.nn.Parameter(lora_B.bias[self.intermediate_size:], requires_grad=False)
        mlp.gate_proj = LoraLayer(gate_proj)
        mlp.gate_proj.lora_A.default = gate_up_linear.lora_A.default
        mlp.gate_proj.lora_B.default = gate_proj_lora_B
        mlp.gate_proj.scaling = gate_up_linear.scaling
        mlp.up_proj = LoraLayer(up_proj)
        mlp.up_proj.lora_A.default = gate_up_linear.lora_A.default
        mlp.up_proj.lora_B.default = up_proj_lora_B
        mlp.up_proj.scaling = gate_up_linear.scaling

    def make_mlp_unpacked_regular(self, layer_id, mlp, gate_up_linear, root_input):
        mlp.gate_proj = torch.nn.Linear(in_features=self.hidden_size, out_features=self.intermediate_size)
        mlp.gate_proj.weight = torch.nn.Parameter(gate_up_linear.weight[:self.intermediate_size, :], requires_grad=False)
        mlp.gate_proj.bias = None if gate_up_linear.bias is None else torch.nn.Parameter(gate_up_linear.bias[:self.intermediate_size], requires_grad=False)
        mlp.up_proj = torch.nn.Linear(in_features=self.hidden_size, out_features=self.intermediate_size)
        mlp.up_proj.weight = torch.nn.Parameter(gate_up_linear.weight[self.intermediate_size:, :])
        mlp.up_proj.bias = None if gate_up_linear.bias is None else torch.nn.Parameter(gate_up_linear.bias[self.intermediate_size:], requires_grad=False)

    def make_mlp_proj(self, layer_id, mlp, root_input):
        gate_bias_exists = mlp.gate_proj.bias is not None and torch.count_nonzero(mlp.gate_proj.bias) > 0
        up_bias_exists = mlp.up_proj.bias is not None and torch.count_nonzero(mlp.up_proj.bias) > 0
        down_bias_exists = mlp.down_proj.bias is not None and torch.count_nonzero(mlp.down_proj.bias) > 0
        gate_matmul_basename = f'/model/layers.{layer_id}/mlp/gate_proj/MatMul'
        gate_matmul_name = self.make_matmul(mlp.gate_proj, gate_matmul_basename, root_input)
        gate_name = gate_matmul_name
        if gate_bias_exists:
            gate_add_name = f'/model/layers.{layer_id}/mlp/gate_proj/Add'
            self.make_add_bias(mlp.gate_proj.bias.detach().numpy(), gate_add_name, root_input=f'{gate_name}/output_0')
            gate_name = gate_add_name
        up_matmul_basename = f'/model/layers.{layer_id}/mlp/up_proj/MatMul'
        up_matmul_name = self.make_matmul(mlp.up_proj, up_matmul_basename, root_input)
        up_name = up_matmul_name
        if up_bias_exists:
            up_add_name = f'/model/layers.{layer_id}/mlp/up_proj/Add'
            self.make_add_bias(mlp.up_proj.bias.detach().numpy(), up_add_name, root_input=f'{up_name}/output_0')
            up_name = up_add_name
        act_fn_name = self.make_activation(layer_id, root_input=f'{gate_name}/output_0')
        mul_name = f'/model/layers.{layer_id}/mlp/Mul'
        mul_inputs = [f'{act_fn_name}/output_0', f'{up_name}/output_0']
        self.make_mul(mul_name, mul_inputs, dtype=self.io_dtype, shape=['batch_size', 'sequence_length', self.intermediate_size])
        down_matmul_basename = f'/model/layers.{layer_id}/mlp/down_proj/MatMul'
        down_matmul_name = self.make_matmul(mlp.down_proj, down_matmul_basename, f'{mul_name}/output_0')
        down_name = down_matmul_name
        if down_bias_exists:
            down_add_name = f'/model/layers.{layer_id}/mlp/down_proj/Add'
            self.make_add_bias(mlp.down_proj.bias.detach().numpy(), down_add_name, root_input=f'{down_name}/output_0')
            down_name = down_add_name
        self.layernorm_attrs['skip_input'] = f'{down_name}/output_0'

    def make_mlp_fc(self, layer_id, mlp, root_input):
        fc1_bias_exists = mlp.fc1.bias is not None and torch.count_nonzero(mlp.fc1.bias) > 0
        fc2_bias_exists = mlp.fc2.bias is not None and torch.count_nonzero(mlp.fc2.bias) > 0
        fc1_matmul_basename = f'/model/layers.{layer_id}/mlp/fc1/MatMul'
        fc1_matmul_name = self.make_matmul(mlp.fc1, fc1_matmul_basename, root_input)
        fc1_name = fc1_matmul_name
        if fc1_bias_exists:
            fc1_add_name = f'/model/layers.{layer_id}/mlp/fc1/Add'
            self.make_add_bias(mlp.fc1.bias.detach().numpy(), fc1_add_name, root_input=f'{fc1_name}/output_0')
            fc1_name = fc1_add_name
        act_fn_name = self.make_activation(layer_id, root_input=f'{fc1_name}/output_0')
        fc2_matmul_basename = f'/model/layers.{layer_id}/mlp/fc2/MatMul'
        fc2_matmul_name = self.make_matmul(mlp.fc2, fc2_matmul_basename, root_input=f'{act_fn_name}/output_0')
        fc2_name = fc2_matmul_name
        if fc2_bias_exists:
            fc2_add_name = f'/model/layers.{layer_id}/mlp/fc2/Add'
            self.make_add_bias(mlp.fc2.bias.detach().numpy(), fc2_add_name, root_input=f'{fc2_name}/output_0')
            fc2_name = fc2_add_name
        self.mlp_attrs['output_0'] = f'{fc2_name}/output_0'

    def make_block_sparse_moe(self, layer_id, bsm, root_input):
        num_experts = self.moe_attrs['num_experts']
        top_k = self.moe_attrs['top_k']
        activation_type = self.moe_attrs['activation_type']
        normalize_routing_weights = self.moe_attrs['normalize_routing_weights']
        use_sparse_mixer = self.moe_attrs['use_sparse_mixer']
        use_int4 = self.moe_attrs['use_int4']
        moe_name = f'/model/layers.{layer_id}/moe'
        gate_ops_base = f'{moe_name}/gate'
        gate_name = f'{gate_ops_base}/MatMul'
        self.make_matmul(bsm.gate, gate_name, root_input)
        shape_name = f'{gate_ops_base}/Shape'
        self.make_shape(shape_name, f'{gate_name}/output_0', shape=[3])
        gather_name = f'{gate_ops_base}/Gather'
        self.make_gather(gather_name, [f'{shape_name}/output_0', '/model/constants/TensorProto.INT64/0D/2'], axis=0)
        unsqueeze_name = f'{gate_ops_base}/Unsqueeze'
        self.make_unsqueeze(unsqueeze_name, [f'{gather_name}/output_0', '/model/constants/TensorProto.INT64/1D/0'], dtype=TensorProto.INT64, shape=[1])
        concat_name = f'{gate_ops_base}/Concat'
        self.make_concat(concat_name, ['/model/constants/TensorProto.INT64/1D/-1', f'{unsqueeze_name}/output_0'], dtype=TensorProto.INT64, shape=[2], axis=0)
        gate_reshape_name = f'{gate_ops_base}/Reshape'
        self.make_reshape(gate_reshape_name, [f'{gate_name}/output_0', f'{concat_name}/output_0'], dtype=self.io_dtype, shape=['num_rows', num_experts])

        def quant_dequant(weights, quant_mode: bool=True):
            type = torch.quint4x2 if quant_mode else torch.int8
            processed_q_weight = None
            torch_weight_scales = None
            try:
                import tensorrt_llm
                (_, processed_q_weight, torch_weight_scales) = torch.ops.trtllm._symmetric_quantize_last_axis_of_batched_matrix(weights.T.cpu().contiguous(), type)
            except:
                raise RuntimeError('tensorrt_llm is needed to use torch.ops.trtllm._symmetric_quantize_last_axis_of_batched_matrix()')
            return (torch_weight_scales.to(torch.float16), processed_q_weight)
        w1_list = []
        w2_list = []
        w3_list = []
        w1_scale_list = []
        w2_scale_list = []
        w3_scale_list = []
        for i in range(num_experts):
            (w1_scale, pre_qweight1) = quant_dequant(bsm.experts[i].w1.weight, use_int4)
            (w2_scale, pre_qweight2) = quant_dequant(bsm.experts[i].w2.weight, use_int4)
            (w3_scale, pre_qweight3) = quant_dequant(bsm.experts[i].w3.weight, use_int4)
            w1_list.append(pre_qweight1)
            w2_list.append(pre_qweight2)
            w3_list.append(pre_qweight3)
            w1_scale_list.append(w1_scale)
            w2_scale_list.append(w2_scale)
            w3_scale_list.append(w3_scale)
        moe_expert_weight_1_name = f'model.layers.{layer_id}.moe.weight_1'
        moe_expert_weight_2_name = f'model.layers.{layer_id}.moe.weight_2'
        moe_expert_weight_3_name = f'model.layers.{layer_id}.moe.weight_3'
        moe_expert_scales_1_name = f'model.layers.{layer_id}.moe.scales_1'
        moe_expert_scales_2_name = f'model.layers.{layer_id}.moe.scales_2'
        moe_expert_scales_3_name = f'model.layers.{layer_id}.moe.scales_3'

        def make_moe_external_tensor(w_list, moe_expert_name, numpy_type):
            moe_experts_weight = torch.stack(w_list, dim=0).detach().numpy()
            self.make_external_tensor(moe_experts_weight.astype(numpy_type), moe_expert_name)
        make_moe_external_tensor(w1_list, moe_expert_weight_1_name, np.uint8)
        make_moe_external_tensor(w2_list, moe_expert_weight_2_name, np.uint8)
        make_moe_external_tensor(w3_list, moe_expert_weight_3_name, np.uint8)
        make_moe_external_tensor(w1_scale_list, moe_expert_scales_1_name, self.to_numpy_dtype[self.io_dtype])
        make_moe_external_tensor(w2_scale_list, moe_expert_scales_2_name, self.to_numpy_dtype[self.io_dtype])
        make_moe_external_tensor(w3_scale_list, moe_expert_scales_3_name, self.to_numpy_dtype[self.io_dtype])
        bias_ph = ''
        inputs = [root_input, f'{gate_reshape_name}/output_0', moe_expert_weight_1_name, moe_expert_scales_1_name, bias_ph, moe_expert_weight_2_name, moe_expert_scales_2_name, bias_ph, moe_expert_weight_3_name, moe_expert_scales_3_name]
        output = f'{moe_name}/output_0'
        op_type = 'QMoE'
        self.make_node(op_type, inputs=inputs, outputs=[output], name=moe_name, domain='com.microsoft', k=top_k, activation_type=activation_type, normalize_routing_weights=normalize_routing_weights, use_sparse_mixer=use_sparse_mixer, expert_weight_bits=4 if use_int4 else 8)
        self.make_value_info(output, self.io_dtype, shape=['batch_size', 'sequence_length', self.hidden_size])
        self.layernorm_attrs['skip_input'] = output

    def make_activation_with_mul(self, layer_id, root_input, activation, domain):
        act_name = f'/model/layers.{layer_id}/mlp/act_fn/{activation}'
        act_output = f'{act_name}/output_0'
        self.make_node(activation, inputs=[root_input], outputs=[act_output], name=act_name, domain=domain)
        self.make_value_info(act_output, dtype=self.io_dtype, shape=['batch_size', 'sequence_length', self.intermediate_size])
        mul_act_name = f'/model/layers.{layer_id}/mlp/act_fn/Mul'
        mul_act_inputs = [root_input, act_output]
        self.make_mul(mul_act_name, mul_act_inputs, dtype=self.io_dtype, shape=['batch_size', 'sequence_length', self.intermediate_size])
        return mul_act_name

    def make_gelu(self, layer_id, root_input, activation):
        gelu_name = f'/model/layers.{layer_id}/mlp/act_fn/{activation}'
        output = f'{gelu_name}/output_0'
        self.make_node(activation, inputs=[root_input], outputs=[output], name=gelu_name, domain='com.microsoft')
        self.make_value_info(output, self.io_dtype, shape=['batch_size', 'sequence_length', self.intermediate_size])
        return gelu_name

    def make_relu(self, layer_id, root_input, activation):
        relu_name = f'/model/layers.{layer_id}/mlp/act_fn/{activation}'
        output = f'{relu_name}/output_0'
        self.make_node(activation, inputs=[root_input], outputs=[output], name=relu_name, domain='')
        self.make_value_info(output, self.io_dtype, shape=['batch_size', 'sequence_length', self.intermediate_size])
        return relu_name

    def make_relu_squared(self, layer_id, root_input, activation):
        relu_name = self.make_relu(layer_id, root_input, 'Relu')
        basename = f'/model/layers.{layer_id}/mlp/square/{activation}'
        pow_name = f'{basename}/pow'
        pow_inputs = [f'{relu_name}/output_0', '/model/constants/TensorProto.INT32/1D/2']
        self.make_node('Pow', inputs=pow_inputs, outputs=[f'{pow_name}/output_0'], name=pow_name, domain='')
        self.make_value_info(f'{pow_name}/output_0', self.io_dtype, shape=['batch_size', 'sequence_length', self.intermediate_size])
        return pow_name

    def make_activation(self, layer_id, root_input):
        if self.activation in {'silu', 'swish', 'swiglu'}:
            output_name = self.make_activation_with_mul(layer_id, root_input, activation='Sigmoid', domain=None)
        elif self.activation in {'gelu_new', 'gelu_fast', 'gelu_pytorch_tanh'}:
            output_name = self.make_gelu(layer_id, root_input, activation='FastGelu')
        elif self.activation in {'gelu'}:
            output_name = self.make_gelu(layer_id, root_input, activation='Gelu')
        elif self.activation in {'gegelu', 'geglu'}:
            output_name = self.make_gelu(layer_id, root_input, activation='QuickGelu')
        elif self.activation in {'relu'}:
            output_name = self.make_relu(layer_id, root_input, activation='Relu')
        elif self.activation in {'relu2'}:
            output_name = self.make_relu_squared(layer_id, root_input, activation='Relu2')
        else:
            raise NotImplementedError(f'The {self.activation} activation function is not currently supported.')
        return output_name

    def make_lm_head(self, lm_head):
        bias_exists = lm_head.bias is not None
        scale_exists = self.lm_head_attrs['scale'] != 1
        mask_exists = self.lm_head_attrs['mask'] is not None
        softcap_exists = self.lm_head_attrs['softcap'] != 0.0
        matmul_basename = '/lm_head/MatMul'
        root_input = self.layernorm_attrs['output_0']
        matmul_name = self.make_matmul(lm_head, matmul_basename, root_input, logits=not (bias_exists or scale_exists or mask_exists or softcap_exists))
        lm_name = matmul_name
        if bias_exists:
            add_name = '/lm_head/Add'
            self.make_add_bias(lm_head.bias.detach().numpy(), add_name, root_input=f'{lm_name}/output_0', logits=not (scale_exists or mask_exists or softcap_exists))
            lm_name = add_name
        if scale_exists:
            mul_name = '/lm_head/Mul'
            mul_inputs = [f'{lm_name}/output_0', f"/model/constants/{self.to_str_dtype[self.io_dtype]}/0D/{self.lm_head_attrs['scale']}"]
            mul_output = 'logits' if not (mask_exists or softcap_exists) else f'{mul_name}/output_0'
            self.make_node('Mul', inputs=mul_inputs, outputs=[mul_output], name=mul_name)
            self.make_value_info(mul_output, self.io_dtype, shape=['batch_size', 'sequence_length', self.vocab_size])
            lm_name = mul_name
        if mask_exists:
            logits_mask_name = 'logits_mask'
            self.make_external_tensor(self.lm_head_attrs['mask'].detach().numpy(), logits_mask_name)
            where_name = '/lm_head/Where'
            where_inputs = [logits_mask_name, f'/model/constants/{self.to_str_dtype[self.io_dtype]}/0D/{np.finfo(self.to_numpy_dtype[self.io_dtype]).min}', f'{lm_name}/output_0']
            where_output = 'logits' if not softcap_exists else f'{where_name}/output_0'
            self.make_node('Where', inputs=where_inputs, outputs=[where_output], name=where_name)
            self.make_value_info(where_output, self.io_dtype, shape=['batch_size', 'sequence_length', self.vocab_size])
            lm_name = where_name
        if softcap_exists:
            div_name = '/lm_head/softcap/Div'
            div_inputs = [f'{lm_name}/output_0', f"/model/constants/{self.to_str_dtype[self.io_dtype]}/0D/{self.lm_head_attrs['softcap']}"]
            self.make_div(div_name, div_inputs, dtype=self.io_dtype, shape=['batch_size', 'sequence_length', self.vocab_size])
            tanh_name = '/lm_head/softcap/Tanh'
            self.make_tanh(tanh_name, f'{div_name}/output_0', dtype=self.io_dtype, shape=['batch_size', 'sequence_length', self.vocab_size])
            mul_name = '/lm_head/softcap/Mul'
            mul_inputs = [f'{tanh_name}/output_0', f"/model/constants/{self.to_str_dtype[self.io_dtype]}/0D/{self.lm_head_attrs['softcap']}"]
            mul_output = 'logits'
            self.make_node('Mul', inputs=mul_inputs, outputs=[mul_output], name=mul_name)
            self.make_value_info(mul_output, self.io_dtype, shape=['batch_size', 'sequence_length', self.vocab_size])
            lm_head = mul_name

    def make_layer(self, layer_id, layer):
        self.make_layernorm(layer_id, layer.input_layernorm, skip=not self.layernorm_attrs['first_layernorm'], simple=self.layernorm_attrs['simple'], location='input')
        self.make_attention(layer_id, layer.self_attn, root_input=self.layernorm_attrs['output_0'])
        self.make_layernorm(layer_id, layer.post_attention_layernorm, skip=True, simple=self.layernorm_attrs['simple'], location='post_attention')
        self.make_mlp(layer_id, layer.mlp, root_input=self.layernorm_attrs['output_0'])
        self.layernorm_attrs['first_layernorm'] = False
        if layer_id == self.num_layers - 1:
            self.layernorm_attrs['last_layernorm'] = True

    def make_model(self, input_path):
        self.make_inputs_and_outputs()
        self.make_preprocessing_nodes()
        if input_path.endswith('.gguf'):
            try:
                from gguf_model import GGUFModel
            except:
                from onnxruntime_genai.models.gguf_model import GGUFModel
            model = GGUFModel.from_pretrained(self.model_type, input_path, self.head_size, self.hidden_size, self.intermediate_size, self.num_attn_heads, self.num_kv_heads, self.vocab_size)
            self.layernorm_attrs['add_offset'] = 0
        elif self.quant_type is not None:
            try:
                from quantized_model import QuantModel
            except:
                from onnxruntime_genai.models.quantized_model import QuantModel
            q_size = self.num_attn_heads * self.head_size
            kv_size = self.num_kv_heads * self.head_size
            model = QuantModel.from_pretrained(self.quant_type, input_path=input_path, quant_attrs=self.quant_attrs, q_size=q_size, kv_size=kv_size, intermediate_size=self.intermediate_size, num_layers=self.num_layers)
        else:
            extra_kwargs = {'num_hidden_layers': self.num_layers} if 'num_hidden_layers' in self.extra_options else {}
            model = AutoModelForCausalLM.from_pretrained(self.model_name_or_path, cache_dir=self.cache_dir, token=self.hf_token, trust_remote_code=True, **extra_kwargs)
        if 'adapter_path' in self.extra_options:
            from peft import PeftModel
            model = PeftModel.from_pretrained(model, self.extra_options['adapter_path'], cache_dir=self.cache_dir, token=self.hf_token)
        self.layer_id = 0
        for module in model.modules():
            if isinstance(module, torch.nn.Embedding) or (hasattr(model, 'embedding') and module == model.embedding):
                if not self.exclude_embeds:
                    print('Reading embedding layer')
                    self.make_embedding(module.weight.detach().numpy())
                else:
                    self.layernorm_attrs['root_input'] = 'inputs_embeds'
                    self.layernorm_attrs['skip_input'] = 'inputs_embeds'
            elif (module.__class__.__name__.endswith('DecoderLayer') or module.__class__.__name__.endswith('GLMBlock')) and self.layer_id < self.num_layers:
                print(f'Reading decoder layer {self.layer_id}')
                self.make_layer(self.layer_id, module)
                self.layer_id += 1
            elif self.layer_id == self.num_layers and self.has_final_norm(module, model):
                print('Reading final norm')
                self.make_layernorm(self.layer_id, module, skip=True, simple=self.layernorm_attrs['simple'], location='final_norm')
            elif isinstance(module, torch.nn.Linear) and module.out_features == self.vocab_size or (hasattr(model, 'lm_head') and module == model.lm_head):
                if not self.exclude_lm_head:
                    print('Reading LM head')
                    self.make_lm_head(module)
        del model

    def has_final_norm(self, module, model):
        hf_norm = hasattr(model, 'model') and hasattr(model.model, 'norm') and (module == model.model.norm)
        hf_final_layernorm = hasattr(model, 'model') and hasattr(model.model, 'final_layernorm') and (module == model.model.final_layernorm)
        hf_transformer_final_layernorm = hasattr(model, 'transformer') and hasattr(model.transformer, 'encoder') and hasattr(model.transformer.encoder, 'final_layernorm') and (module == model.transformer.encoder.final_layernorm)
        hf_multimodal_final_layernorm = hasattr(model, 'language_model') and hasattr(model.language_model, 'model') and hasattr(model.language_model.model, 'norm') and (module == model.language_model.model.norm)
        gguf_final_norm = hasattr(model, 'final_norm') and module == model.final_norm
        return hf_norm or hf_final_layernorm or hf_transformer_final_layernorm or hf_multimodal_final_layernorm or gguf_final_norm

    def make_preprocessing_nodes(self):
        self.make_attention_mask_reformatting()

    def make_attention_mask_reformatting(self):
        if self.extra_options.get('enable_cuda_graph', False) or self.ep == 'dml':
            assert self.past_present_share_buffer
        if self.attention_attrs['op_type'] == 'GroupQueryAttention':
            self.make_attention_mask_reformatting_for_gqa()
        elif self.attention_attrs['op_type'] == 'MultiHeadAttention':
            self.make_attention_mask_reformatting_for_mha()
        if self.attention_attrs['block_sparse']['sparse_block_size'] != 0:
            self.make_attention_mask_reformatting_for_sparse_attn()

    def make_attention_mask_reformatting_for_mha(self):
        basename = '/model/attn_mask_reformat'
        input_ids_basename = f'{basename}/input_ids_subgraph'
        past_key_basename = f'{basename}/past_key_subgraph'
        attn_mask_basename = f'{basename}/attn_mask_subgraph'
        past_key_gather_name = self.make_past_key_subgraph(past_key_basename)
        (shared_unsqueeze_name, end_expand_name) = self.make_input_ids_subgraph(input_ids_basename, past_key_gather_name)
        end_where_name = self.make_attention_mask_subgraph(attn_mask_basename, shared_unsqueeze_name)
        end_add_name = f'{basename}/Add'
        end_add_inputs = [f'{end_where_name}/output_0', f'{end_expand_name}/output_0']
        end_add_shape = ['batch_size', 1, 'source_sequence_length', 'target_sequence_length']
        self.make_add(end_add_name, end_add_inputs, dtype=self.io_dtype, shape=end_add_shape)
        tile_name = f'{basename}/Tile'
        tile_inputs = [f'{end_add_name}/output_0', f'/model/constants/TensorProto.INT64/1D/1, {self.num_attn_heads}, 1, 1']
        tile_shape = ['batch_size', self.num_attn_heads, 'source_sequence_length', 'target_sequence_length']
        self.make_tile(tile_name, tile_inputs, dtype=self.io_dtype, shape=tile_shape)
        self.mask_attrs['mask_name'] = tile_name

    def make_past_key_subgraph(self, basename):
        shape_name = f'{basename}/Shape'
        self.make_shape(shape_name, 'past_key_values.0.key', shape=[4])
        gather_name = f'{basename}/Gather'
        gather_inputs = [f'{shape_name}/output_0', '/model/constants/TensorProto.INT64/0D/2']
        self.make_gather(gather_name, gather_inputs, axis=0)
        return gather_name

    def make_input_ids_subgraph(self, basename, past_key_gather_name):
        shared_add_name = f'{basename}/Add_1'
        shared_add_inputs = [f'{basename}/Gather_2/output_0', f'{past_key_gather_name}/output_0']
        self.make_add(shared_add_name, shared_add_inputs, dtype=TensorProto.INT64, shape=[])
        unsqueeze_3_name = f'{basename}/Unsqueeze_3'
        unsqueeze_3_inputs = [f'{shared_add_name}/output_0', '/model/constants/TensorProto.INT64/1D/0']
        self.make_unsqueeze(unsqueeze_3_name, unsqueeze_3_inputs, dtype=TensorProto.INT64, shape=[1])
        unsqueeze_inputs = [f'{basename}/Gather_2/output_0', '/model/constants/TensorProto.INT64/1D/0']
        unsqueeze_4_name = f'{basename}/Unsqueeze_4'
        self.make_unsqueeze(unsqueeze_4_name, unsqueeze_inputs, dtype=TensorProto.INT64, shape=[1])
        unsqueeze_5_name = f'{basename}/Unsqueeze_5'
        self.make_unsqueeze(unsqueeze_5_name, unsqueeze_inputs, dtype=TensorProto.INT64, shape=[1])
        unsqueeze_6_name = f'{basename}/Unsqueeze_6'
        self.make_unsqueeze(unsqueeze_6_name, unsqueeze_inputs, dtype=TensorProto.INT64, shape=[1])
        concat_2_name = f'{basename}/Concat_2'
        concat_inputs = [f'{unsqueeze_4_name}/output_0', f'{unsqueeze_5_name}/output_0']
        self.make_concat(concat_2_name, concat_inputs, dtype=TensorProto.INT64, shape=[2], axis=0)
        constant_shape_name = f'{basename}/ConstantOfShape_2'
        constant_shape_numpy_dtype = self.to_numpy_dtype[self.io_dtype]
        constant_shape_value = numpy_helper.from_array(np.array([np.finfo(constant_shape_numpy_dtype).min], dtype=constant_shape_numpy_dtype))
        self.make_constant_of_shape(constant_shape_name, f'{concat_2_name}/output_0', value=constant_shape_value, dtype=self.io_dtype, shape=['unk', 'unk'])
        shape_4_name = f'{basename}/Shape_4'
        self.make_shape(shape_4_name, f'{constant_shape_name}/output_0', shape=[2])
        slice_1_name = f'{basename}/Slice_1'
        slice_1_inputs = [f'{shape_4_name}/output_0', '/model/constants/TensorProto.INT64/1D/-1', f'/model/constants/TensorProto.INT64/1D/{np.iinfo(np.int64).max}', '/model/constants/TensorProto.INT64/1D/0']
        self.make_slice(slice_1_name, slice_1_inputs, dtype=TensorProto.INT64, shape=[1])
        squeeze_1_name = f'{basename}/Squeeze_1'
        squeeze_1_inputs = [f'{slice_1_name}/output_0', '/model/constants/TensorProto.INT64/1D/0']
        self.make_squeeze(squeeze_1_name, squeeze_1_inputs)
        unsqueeze_7_name = f'{basename}/output_0'
        unsqueeze_7_inputs = [f'{squeeze_1_name}/output_0', '/model/constants/TensorProto.INT64/1D/0']
        self.make_unsqueeze(unsqueeze_7_name, unsqueeze_7_inputs, dtype=TensorProto.INT64, shape=[1])
        concat_3_name = f'{basename}/Concat_3'
        concat_3_inputs = [f'{unsqueeze_7_name}/output_0', '/model/constants/TensorProto.INT64/1D/1']
        self.make_concat(concat_3_name, concat_3_inputs, dtype=TensorProto.INT64, shape=[2], axis=0)
        shape_5_name = f'{basename}/Shape_5'
        self.make_shape(shape_5_name, f'{constant_shape_name}/output_0', shape=[2])
        slice_2_name = f'{basename}/Slice_2'
        slice_2_inputs = [f'{shape_5_name}/output_0', '/model/constants/TensorProto.INT64/1D/-1', f'/model/constants/TensorProto.INT64/1D/{np.iinfo(np.int64).max}', '/model/constants/TensorProto.INT64/1D/0']
        self.make_slice(slice_2_name, slice_2_inputs, dtype=TensorProto.INT64, shape=[1])
        squeeze_2_name = f'{basename}/Squeeze_2'
        squeeze_2_inputs = [f'{slice_2_name}/output_0', '/model/constants/TensorProto.INT64/1D/0']
        self.make_squeeze(squeeze_2_name, squeeze_2_inputs)
        range_name = f'{basename}/Range'
        range_inputs = ['/model/constants/TensorProto.INT64/0D/0', f'{squeeze_2_name}/output_0', '/model/constants/TensorProto.INT64/0D/1']
        self.make_range(range_name, range_inputs)
        add_2_name = f'{basename}/Add_2'
        add_inputs = [f'{range_name}/output_0', '/model/constants/TensorProto.INT64/0D/1']
        self.make_add(add_2_name, add_inputs, dtype=TensorProto.INT64, shape=['unk'])
        reshape_name = f'{basename}/Reshape'
        reshape_inputs = [f'{add_2_name}/output_0', f'{concat_3_name}/output_0']
        self.make_reshape(reshape_name, reshape_inputs, dtype=TensorProto.INT64, shape=None)
        less_name = f'{basename}/Less'
        less_inputs = [f'{range_name}/output_0', f'{reshape_name}/output_0']
        self.make_less(less_name, less_inputs)
        where_2_name = f'{basename}/Where_2'
        where_2_inputs = [f'{less_name}/output_0', f'/model/constants/{self.to_str_dtype[self.io_dtype]}/0D/0', f'{constant_shape_name}/output_0']
        self.make_where(where_2_name, where_2_inputs, dtype=self.io_dtype, shape=None)
        unsqueeze_8_name = f'{basename}/Unsqueeze_8'
        unsqueeze_8_inputs = [f'{where_2_name}/output_0', '/model/constants/TensorProto.INT64/1D/0']
        self.make_unsqueeze(unsqueeze_8_name, unsqueeze_8_inputs, dtype=self.io_dtype, shape=None)
        unsqueeze_9_name = f'{basename}/Unsqueeze_9'
        unsqueeze_9_inputs = [f'{unsqueeze_8_name}/output_0', '/model/constants/TensorProto.INT64/1D/1']
        self.make_unsqueeze(unsqueeze_9_name, unsqueeze_9_inputs, dtype=self.io_dtype, shape=None)
        expand_name = self.make_common_mask_reformat_subgraph(basename, root_input='input_ids' if not self.exclude_embeds else 'inputs_embeds', unsqueeze_for_concat=unsqueeze_3_name, unsqueeze_for_expand=unsqueeze_9_name, input_ids_subgraph=True)
        return (unsqueeze_6_name, expand_name)

    def make_attention_mask_subgraph(self, basename, unsqueeze_for_concat):
        attention_mask_shape = self.input_shapes['attention_mask']
        unsqueeze_3_name = f'{basename}/Unsqueeze_3'
        unsqueeze_3_inputs = ['attention_mask', '/model/constants/TensorProto.INT64/1D/1']
        attention_mask_shape.insert(1, 1)
        self.make_unsqueeze(unsqueeze_3_name, unsqueeze_3_inputs, dtype=TensorProto.INT64, shape=attention_mask_shape)
        unsqueeze_4_name = f'{basename}/Unsqueeze_4'
        unsqueeze_4_inputs = [f'{unsqueeze_3_name}/output_0', '/model/constants/TensorProto.INT64/1D/2']
        attention_mask_shape.insert(1, 1)
        self.make_unsqueeze(unsqueeze_4_name, unsqueeze_4_inputs, dtype=TensorProto.INT64, shape=attention_mask_shape)
        expand_name = self.make_common_mask_reformat_subgraph(basename, root_input='attention_mask', unsqueeze_for_concat=unsqueeze_for_concat, unsqueeze_for_expand=unsqueeze_4_name)
        cast_1_name = f'{basename}/Cast_1'
        self.make_cast(cast_1_name, f'{expand_name}/output_0', dtype=self.io_dtype, shape=['unk', 'unk', 'unk', 'unk'])
        sub_name = f'{basename}/Sub'
        sub_inputs = [f'/model/constants/{self.to_str_dtype[self.io_dtype]}/0D/1', f'{cast_1_name}/output_0']
        self.make_sub(sub_name, sub_inputs, dtype=self.io_dtype, shape=['unk', 'unk', 'unk', 'unk'])
        cast_2_name = f'{basename}/Cast_2'
        self.make_cast(cast_2_name, f'{sub_name}/output_0', dtype=TensorProto.BOOL, shape=['unk', 'unk', 'unk', 'unk'])
        where_2_name = f'{basename}/Where_2'
        where_2_inputs = [f'{cast_2_name}/output_0', f'/model/constants/{self.to_str_dtype[self.io_dtype]}/0D/{np.finfo(self.to_numpy_dtype[self.io_dtype]).min}', f'{sub_name}/output_0']
        self.make_where(where_2_name, where_2_inputs, dtype=self.io_dtype, shape=['unk', 'unk', 'unk', 'unk'])
        return where_2_name

    def make_common_mask_reformat_subgraph(self, basename, root_input, unsqueeze_for_concat, unsqueeze_for_expand, input_ids_subgraph=False):
        shape_1_name = f'{basename}/Shape_1'
        self.make_shape(shape_1_name, root_input, shape=[3] if self.exclude_embeds and input_ids_subgraph else [2])
        shape_2_name = f'{basename}/Shape_2'
        self.make_shape(shape_2_name, root_input, shape=[3] if self.exclude_embeds and input_ids_subgraph else [2])
        gather_1_name = f'{basename}/Gather_1'
        gather_1_inputs = [f'{shape_1_name}/output_0', '/model/constants/TensorProto.INT64/0D/0']
        self.make_gather(gather_1_name, gather_1_inputs, axis=0)
        gather_2_name = f'{basename}/Gather_2'
        gather_2_inputs = [f'{shape_2_name}/output_0', '/model/constants/TensorProto.INT64/0D/1']
        self.make_gather(gather_2_name, gather_2_inputs, axis=0)
        unsqueeze_1_name = f'{basename}/Unsqueeze_1'
        unsqueeze_1_inputs = [f'{gather_1_name}/output_0', '/model/constants/TensorProto.INT64/1D/0']
        self.make_unsqueeze(unsqueeze_1_name, unsqueeze_1_inputs, dtype=TensorProto.INT64, shape=[1])
        unsqueeze_2_name = f'{basename}/Unsqueeze_2'
        unsqueeze_2_inputs = [f'{gather_2_name}/output_0', '/model/constants/TensorProto.INT64/1D/0']
        self.make_unsqueeze(unsqueeze_2_name, unsqueeze_2_inputs, dtype=TensorProto.INT64, shape=[1])
        concat_name = f'{basename}/Concat' if not input_ids_subgraph else f'{basename}/Concat_1'
        concat_first_two_inputs = [f'{unsqueeze_1_name}/output_0', '/model/constants/TensorProto.INT64/1D/1']
        concat_last_two_inputs = [f'{unsqueeze_for_concat}/output_0', f'{unsqueeze_2_name}/output_0'] if not input_ids_subgraph else [f'{unsqueeze_2_name}/output_0', f'{unsqueeze_for_concat}/output_0']
        concat_inputs = concat_first_two_inputs + concat_last_two_inputs
        self.make_concat(concat_name, concat_inputs, dtype=TensorProto.INT64, shape=[4], axis=0)
        shape_3_name = f'{basename}/Shape_3'
        self.make_shape(shape_3_name, f'{concat_name}/output_0', shape=[1])
        constant_shape_name = f'{basename}/ConstantOfShape' if not input_ids_subgraph else f'{basename}/ConstantOfShape_1'
        constant_shape_value = numpy_helper.from_array(np.array([1], dtype='int64'))
        self.make_constant_of_shape(constant_shape_name, f'{shape_3_name}/output_0', value=constant_shape_value, dtype=TensorProto.INT64, shape=['unk'])
        mul_name = f'{basename}/Mul'
        mul_inputs = [f'{constant_shape_name}/output_0', '/model/constants/TensorProto.INT64/0D/-1']
        self.make_mul(mul_name, mul_inputs, dtype=TensorProto.INT64, shape=['unk'])
        equal_name = f'{basename}/Equal'
        equal_inputs = [f'{concat_name}/output_0', f'{mul_name}/output_0']
        self.make_equal(equal_name, equal_inputs, shape=[4])
        where_name = f'{basename}/Where_1'
        where_inputs = [f'{equal_name}/output_0', f'{constant_shape_name}/output_0', f'{concat_name}/output_0']
        self.make_where(where_name, where_inputs, dtype=TensorProto.INT64, shape=[4])
        expand_name = f'{basename}/Expand'
        expand_inputs = [f'{unsqueeze_for_expand}/output_0', f'{where_name}/output_0']
        expand_dtype = self.io_dtype if input_ids_subgraph else TensorProto.INT64
        expand_shape = None if input_ids_subgraph else ['unk', 'unk', 'unk', 'unk']
        self.make_expand(expand_name, expand_inputs, dtype=expand_dtype, shape=expand_shape)
        return expand_name

    def make_attention_mask_reformatting_for_gqa(self):
        basename = '/model/attn_mask_reformat'
        attn_mask_basename = f'{basename}/attn_mask_subgraph'
        reduce_sum_name = f'{attn_mask_basename}/ReduceSum'
        reduce_sum_inputs = ['attention_mask', '/model/constants/TensorProto.INT64/1D/1']
        self.make_reduce_sum(reduce_sum_name, reduce_sum_inputs, dtype=TensorProto.INT64, shape=['batch_size', 1])
        sub_name = f'{attn_mask_basename}/Sub'
        sub_inputs = [f'{reduce_sum_name}/output_0', '/model/constants/TensorProto.INT64/1D/1']
        self.make_sub(sub_name, sub_inputs, dtype=TensorProto.INT64, shape=['batch_size', 1])
        cast_1_name = f'{attn_mask_basename}/Sub/Cast'
        self.make_cast(cast_1_name, f'{sub_name}/output_0', dtype=TensorProto.INT32, shape=['batch_size', 1])
        shape_name = f'{attn_mask_basename}/Shape'
        self.make_shape(shape_name, 'attention_mask', shape=[2])
        gather_name = f'{attn_mask_basename}/Gather'
        gather_inputs = [f'{shape_name}/output_0', '/model/constants/TensorProto.INT64/0D/1']
        self.make_gather(gather_name, gather_inputs, axis=0)
        cast_2_name = f'{attn_mask_basename}/Gather/Cast'
        self.make_cast(cast_2_name, f'{gather_name}/output_0', dtype=TensorProto.INT32, shape=None)
        self.mask_attrs['seqlens_k'] = cast_1_name
        self.mask_attrs['total_seq_len'] = cast_2_name

    def make_attention_mask_reformatting_for_sparse_attn(self):
        basename = '/model/attn_mask_reformat'
        attn_mask_basename = f'{basename}/attn_mask_subgraph'
        reduce_sum_name = f'{attn_mask_basename}/ReduceSum'
        reduce_sum_inputs = ['attention_mask', '/model/constants/TensorProto.INT64/1D/1']
        self.make_reduce_sum(reduce_sum_name, reduce_sum_inputs, dtype=TensorProto.INT64, shape=['batch_size', 1])
        cast_1_name = f'{attn_mask_basename}/ReduceSum/Cast'
        self.make_cast(cast_1_name, f'{reduce_sum_name}/output_0', dtype=TensorProto.INT32, shape=['batch_size', 1])
        shape_name = f'{attn_mask_basename}/Shape'
        self.make_shape(shape_name, 'attention_mask', shape=[2])
        gather_name = f'{attn_mask_basename}/Gather'
        gather_inputs = [f'{shape_name}/output_0', '/model/constants/TensorProto.INT64/0D/1']
        self.make_gather(gather_name, gather_inputs, axis=0)
        cast_2_name = f'{attn_mask_basename}/Gather/Cast'
        self.make_cast(cast_2_name, f'{gather_name}/output_0', dtype=TensorProto.INT32, shape=None)
        self.mask_attrs['key_total_seq_lens'] = cast_1_name
        self.mask_attrs['total_seq_len'] = cast_2_name

    def make_position_ids_reformatting(self):
        basename = '/model/pos_ids_reformat'
        proto_dtype = self.input_types['position_ids']
        str_dtype = self.to_str_dtype[proto_dtype]
        shape_name = f'{basename}/Shape'
        self.make_shape(shape_name, root_input='input_ids' if not self.exclude_embeds else 'inputs_embeds', shape=[2] if not self.exclude_embeds else [3])
        gather_name = f'{basename}/Gather'
        gather_inputs = [f'{shape_name}/output_0', f'/model/constants/{str_dtype}/0D/1']
        self.make_gather(gather_name, gather_inputs, axis=0)
        unsqueeze_name = f'{basename}/Unsqueeze'
        unsqueeze_inputs = [f'{gather_name}/output_0', f'/model/constants/{str_dtype}/1D/0']
        self.make_unsqueeze(unsqueeze_name, unsqueeze_inputs, dtype=proto_dtype, shape=[1])
        concat_name = f'{basename}/Concat'
        concat_inputs = [f'/model/constants/{str_dtype}/1D/-1', f'{unsqueeze_name}/output_0']
        self.make_concat(concat_name, concat_inputs, dtype=proto_dtype, shape=[2], axis=0)
        reshape_name = f'{basename}/Reshape'
        reshape_inputs = ['position_ids', f'{concat_name}/output_0']
        self.make_reshape(reshape_name, reshape_inputs, dtype=proto_dtype, shape=None)
        return reshape_name

class MistralModel(Model):

    def __init__(self, config, io_dtype, onnx_dtype, ep, cache_dir, extra_options):
        super().__init__(config, io_dtype, onnx_dtype, ep, cache_dir, extra_options)
        self.position_ids_name = f'{self.make_position_ids_reformatting()}/output_0' if not self.attention_attrs['use_rope_in_attn'] else 'position_ids'

    def make_attention(self, layer_id, attention, root_input, **kwargs):
        super().make_attention(layer_id, attention, root_input, position_ids=self.position_ids_name, **kwargs)

class Qwen3Model(QwenModel):

    def __init__(self, config, io_dtype, onnx_dtype, ep, cache_dir, extra_options):
        super().__init__(config, io_dtype, onnx_dtype, ep, cache_dir, extra_options)

    def make_attention_init(self):
        self.attention_attrs['q_norm'] = True
        self.attention_attrs['k_norm'] = True
        super().make_attention_init()

class GemmaModel(MistralModel):

    def __init__(self, config, io_dtype, onnx_dtype, ep, cache_dir, extra_options):
        super().__init__(config, io_dtype, onnx_dtype, ep, cache_dir, extra_options)
        self.embed_attrs['scale'] = np.round(np.sqrt(self.hidden_size), decimals=2)
        self.layernorm_attrs['add_offset'] = 1

class Gemma2Model(GemmaModel):

    def __init__(self, config, io_dtype, onnx_dtype, ep, cache_dir, extra_options):
        super().__init__(config, io_dtype, onnx_dtype, ep, cache_dir, extra_options)
        self.attention_attrs['scale'] = config.query_pre_attn_scalar ** (-0.5)
        self.is_local = lambda layer_id: layer_id % 2 == 1

    def make_layer(self, layer_id, layer):
        self.make_layernorm(layer_id, layer.input_layernorm, skip=not self.layernorm_attrs['first_layernorm'], simple=self.layernorm_attrs['simple'], location='input')
        self.make_attention(layer_id, layer.self_attn, root_input=self.layernorm_attrs['output_0'])
        original_root_input = self.layernorm_attrs['root_input']
        self.layernorm_attrs['root_input'] = self.layernorm_attrs['skip_input']
        self.make_layernorm(layer_id, layer.post_attention_layernorm, skip=False, simple=self.layernorm_attrs['simple'], location='post_attention')
        self.layernorm_attrs['root_input'] = original_root_input
        self.layernorm_attrs['skip_input'] = self.layernorm_attrs['output_0']
        self.make_layernorm(layer_id, layer.pre_feedforward_layernorm, skip=True, simple=self.layernorm_attrs['simple'], location='pre_feedforward')
        self.make_mlp(layer_id, layer.mlp, root_input=self.layernorm_attrs['output_0'])
        original_root_input = self.layernorm_attrs['root_input']
        self.layernorm_attrs['root_input'] = self.layernorm_attrs['skip_input']
        self.make_layernorm(layer_id, layer.post_feedforward_layernorm, skip=False, simple=self.layernorm_attrs['simple'], location='post_feedforward')
        self.layernorm_attrs['root_input'] = original_root_input
        self.layernorm_attrs['skip_input'] = self.layernorm_attrs['output_0']
        self.layernorm_attrs['first_layernorm'] = False
        if layer_id == self.num_layers - 1:
            self.layernorm_attrs['last_layernorm'] = True

    def make_attention(self, layer_id, attention, root_input, **kwargs):
        original_window_size = self.window_size
        self.window_size = original_window_size if self.is_local(layer_id) else -1
        super().make_attention(layer_id, attention, root_input, **kwargs)
        self.window_size = original_window_size

class Gemma3Model(Gemma2Model):

    def __init__(self, config, io_dtype, onnx_dtype, ep, cache_dir, extra_options):
        super().__init__(config, io_dtype, onnx_dtype, ep, cache_dir, extra_options)
        self.is_local = lambda layer_id: bool((layer_id + 1) % config.sliding_window_pattern)
        self.rope_local_theta = config.rope_local_base_freq
        self.make_rotary_embedding_multi_cache()

    def make_attention_init(self):
        self.attention_attrs['q_norm'] = True
        self.attention_attrs['k_norm'] = True
        super().make_attention_init()

    def make_rotary_embedding_multi_cache(self):
        (self.cos_cache_global_name, self.sin_cache_global_name) = ('cos_cache_global', 'sin_cache_global')
        super().make_rotary_embedding_caches(cos_cache_name=self.cos_cache_global_name, sin_cache_name=self.sin_cache_global_name)
        self.rotemb_attrs['create_caches'] = True
        self.rotemb_attrs['theta'] = self.rope_local_theta
        (self.cos_cache_local_name, self.sin_cache_local_name) = ('cos_cache_local', 'sin_cache_local')
        super().make_rotary_embedding_caches(cos_cache_name=self.cos_cache_local_name, sin_cache_name=self.sin_cache_local_name)

    def make_rotary_embedding_caches(self, **kwargs):
        cos_cache_name = kwargs.get('cos_cache_name', self.cos_cache_global_name if self.window_size == -1 else self.cos_cache_local_name)
        sin_cache_name = kwargs.get('sin_cache_name', self.sin_cache_global_name if self.window_size == -1 else self.sin_cache_local_name)
        return super().make_rotary_embedding_caches(cos_cache_name=cos_cache_name, sin_cache_name=sin_cache_name)

def check_extra_options(kv_pairs):
    """
    Check key-value pairs and set values correctly
    """
    bools = ['int4_is_symmetric', 'exclude_embeds', 'exclude_lm_head', 'include_hidden_states', 'enable_cuda_graph', 'use_8bits_moe', 'use_qdq', 'include_prompt_templates']
    for key in bools:
        if key in kv_pairs:
            if kv_pairs[key] in {'false', 'False', '0'}:
                kv_pairs[key] = False
            elif kv_pairs[key] in {'true', 'True', '1'}:
                kv_pairs[key] = True
            else:
                raise ValueError(f'{key} must be false/False/0 or true/True/1.')
    if 'int4_op_types_to_quantize' in kv_pairs:
        op_types_to_quantize = ()
        for op_type in kv_pairs['int4_op_types_to_quantize'].split('/'):
            op_types_to_quantize += (op_type,)
        kv_pairs['int4_op_types_to_quantize'] = op_types_to_quantize
    if 'int4_nodes_to_exclude' in kv_pairs:
        nodes_to_exclude = []
        for node in kv_pairs['int4_nodes_to_exclude'].split(','):
            nodes_to_exclude.append(node)
        kv_pairs['int4_nodes_to_exclude'] = nodes_to_exclude
    if 'exclude_lm_head' in kv_pairs and 'include_hidden_states' in kv_pairs:
        raise ValueError(f"Both 'exclude_lm_head' and 'include_hidden_states' cannot be used together. Please use only one of them at once.")

def parse_extra_options(kv_items):
    """
    Parse key-value pairs that are separated by '='
    """
    kv_pairs = {}
    if kv_items:
        for kv_str in kv_items:
            kv = kv_str.split('=')
            kv_pairs[kv[0].strip()] = kv[1].strip()
    print(f'Extra options: {kv_pairs}')
    check_extra_options(kv_pairs)
    return kv_pairs

def parse_hf_token(hf_token):
    """
    Returns the authentication token needed for Hugging Face.
    Token is obtained either from the user or the environment.
    """
    if hf_token.lower() in {'false', '0'}:
        return None
    if hf_token.lower() in {'true', '1'}:
        return True
    return hf_token

def create_model(model_name, input_path, output_dir, precision, execution_provider, cache_dir, **extra_options):
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(cache_dir, exist_ok=True)
    extra_kwargs = {} if os.path.isdir(input_path) else {'cache_dir': cache_dir}
    hf_name = input_path if os.path.isdir(input_path) else model_name
    hf_token = parse_hf_token(extra_options.get('hf_token', 'true'))
    config = AutoConfig.from_pretrained(hf_name, token=hf_token, trust_remote_code=True, **extra_kwargs)
    if 'adapter_path' in extra_options:
        from peft import PeftConfig
        peft_config = PeftConfig.from_pretrained(extra_options['adapter_path'], token=hf_token, trust_remote_code=True, **extra_kwargs)
        config.update(peft_config.__dict__)
    use_webgpu_fp32 = extra_options.get('use_webgpu_fp32', '0') == '1'
    io_dtype = TensorProto.FLOAT if precision in {'int8', 'fp32'} or (precision == 'int4' and execution_provider == 'cpu') or use_webgpu_fp32 else TensorProto.FLOAT16
    if 'config_only' not in extra_options:
        if config.architectures[0] == 'ChatGLMForConditionalGeneration' or config.architectures[0] == 'ChatGLMModel':
            config.hidden_act = 'swiglu'
            onnx_model = ChatGLMModel(config, io_dtype, precision, execution_provider, cache_dir, extra_options)
        elif config.architectures[0] == 'GemmaForCausalLM':
            onnx_model = GemmaModel(config, io_dtype, precision, execution_provider, cache_dir, extra_options)
        elif config.architectures[0] == 'Gemma2ForCausalLM':
            onnx_model = Gemma2Model(config, io_dtype, precision, execution_provider, cache_dir, extra_options)
        elif config.architectures[0] == 'Gemma3ForCausalLM':
            onnx_model = Gemma3Model(config, io_dtype, precision, execution_provider, cache_dir, extra_options)
            onnx_model.model_type = 'gemma3_text'
        elif config.architectures[0] == 'Gemma3ForConditionalGeneration':
            print('WARNING: This is only generating the text component of the model. Setting `--extra_options exclude_embeds=true` by default.')
            text_config = config.text_config
            for key in text_config:
                if not hasattr(config, key):
                    setattr(config, key, getattr(text_config, key))
            extra_options['exclude_embeds'] = True
            onnx_model = Gemma3Model(config, io_dtype, precision, execution_provider, cache_dir, extra_options)
        elif config.architectures[0] == 'GraniteForCausalLM':
            onnx_model = GraniteModel(config, io_dtype, precision, execution_provider, cache_dir, extra_options)
        elif config.architectures[0] == 'LlamaForCausalLM':
            onnx_model = LlamaModel(config, io_dtype, precision, execution_provider, cache_dir, extra_options)
        elif config.architectures[0] == 'MistralForCausalLM':
            onnx_model = MistralModel(config, io_dtype, precision, execution_provider, cache_dir, extra_options)
        elif config.architectures[0] == 'NemotronForCausalLM':
            onnx_model = NemotronModel(config, io_dtype, precision, execution_provider, cache_dir, extra_options)
        elif config.architectures[0] == 'OlmoForCausalLM':
            onnx_model = OLMoModel(config, io_dtype, precision, execution_provider, cache_dir, extra_options)
        elif config.architectures[0] == 'PhiForCausalLM':
            onnx_model = PhiModel(config, io_dtype, precision, execution_provider, cache_dir, extra_options)
        elif config.architectures[0] == 'Phi3ForCausalLM' and config.max_position_embeddings == config.original_max_position_embeddings:
            onnx_model = Phi3MiniModel(config, io_dtype, precision, execution_provider, cache_dir, extra_options)
        elif config.architectures[0] == 'Phi3ForCausalLM' and config.max_position_embeddings != config.original_max_position_embeddings:
            onnx_model = Phi3MiniLongRoPEModel(config, io_dtype, precision, execution_provider, cache_dir, extra_options)
        elif config.architectures[0] == 'PhiMoEForCausalLM' and config.max_position_embeddings != config.original_max_position_embeddings:
            print('WARNING: This model only works for CUDA currently because `MoE` is only supported for CUDA in ONNX Runtime. Setting `--execution_provider cuda` by default.')
            print('WARNING: This model currently only supports quantized version. Setting `--precision int4` by default.')
            execution_provider = 'cuda'
            precision = 'int4'
            onnx_model = Phi3MoELongRoPEModel(config, io_dtype, precision, execution_provider, cache_dir, extra_options)
        elif config.architectures[0] == 'Phi3SmallForCausalLM' and config.max_position_embeddings == config.original_max_position_embeddings:
            onnx_model = Phi3SmallModel(config, io_dtype, precision, execution_provider, cache_dir, extra_options)
        elif config.architectures[0] == 'Phi3SmallForCausalLM' and config.max_position_embeddings != config.original_max_position_embeddings:
            onnx_model = Phi3SmallLongRoPEModel(config, io_dtype, precision, execution_provider, cache_dir, extra_options)
        elif config.architectures[0] == 'Phi3VForCausalLM':
            print('WARNING: This is only generating the text component of the model. Setting `--extra_options exclude_embeds=true` by default.')
            extra_options['exclude_embeds'] = True
            onnx_model = Phi3VModel(config, io_dtype, precision, execution_provider, cache_dir, extra_options)
        elif config.architectures[0] == 'Phi4MMForCausalLM':
            print('WARNING: This is only generating the text component of the model. Setting `--extra_options exclude_embeds=true` by default.')
            extra_options['exclude_embeds'] = True
            onnx_model = Phi4MMModel(config, io_dtype, precision, execution_provider, cache_dir, extra_options)
        elif config.architectures[0] == 'Qwen2ForCausalLM':
            onnx_model = QwenModel(config, io_dtype, precision, execution_provider, cache_dir, extra_options)
        elif config.architectures[0] == 'Qwen3ForCausalLM':
            onnx_model = Qwen3Model(config, io_dtype, precision, execution_provider, cache_dir, extra_options)
        else:
            raise NotImplementedError(f'The {hf_name} model is not currently supported.')
        onnx_model.make_model(input_path)
        onnx_model.save_model(output_dir)
    else:
        onnx_model = Model(config, io_dtype, precision, execution_provider, cache_dir, extra_options)
    onnx_model.make_genai_config(hf_name, extra_kwargs, output_dir)
    onnx_model.save_processing(hf_name, extra_kwargs, output_dir)

def get_args():
    parser = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter)
    parser.add_argument('-m', '--model_name', required=False, default=None, help='Model name in Hugging Face. Do not use if providing an input path to a Hugging Face directory in -i/--input.')
    parser.add_argument('-i', '--input', required=False, default='', help=textwrap.dedent('            Input model source. Currently supported options are:\n                hf_path: Path to folder on disk containing the Hugging Face config, model, tokenizer, etc.\n                gguf_path: Path to float16/float32 GGUF file on disk containing the GGUF model\n            '))
    parser.add_argument('-o', '--output', required=True, help='Path to folder to store ONNX model and additional files (e.g. GenAI config, external data files, etc.)')
    parser.add_argument('-p', '--precision', required=True, choices=['int4', 'fp16', 'fp32'], help='Precision of model')
    parser.add_argument('-e', '--execution_provider', required=True, choices=['cpu', 'cuda', 'rocm', 'dml', 'webgpu'], help='Execution provider to target with precision of model (e.g. FP16 CUDA, INT4 CPU, INT4 WEBGPU)')
    parser.add_argument('-c', '--cache_dir', required=False, type=str, default=os.path.join('.', 'cache_dir'), help='Cache directory for Hugging Face files and temporary ONNX external data files')
    parser.add_argument('--extra_options', required=False, metavar='KEY=VALUE', nargs='+', help=textwrap.dedent("            Key value pairs for various options. Currently supports:\n                int4_accuracy_level = 1/2/3/4: Specify the minimum accuracy level for activation of MatMul in int4 quantization.\n                    4 is int8, which means input A of int4 quantized MatMul is quantized to int8 and input B is upcasted to int8 for computation.\n                    3 is bf16.\n                    2 is fp16.\n                    1 is fp32.\n                    Default is 4 for the CPU EP and 0 for non-CPU EPs.\n                int4_block_size = 16/32/64/128/256: Specify the block_size for int4 quantization.\n                int4_is_symmetric = Quantize the weights symmetrically. Default is true.\n                    If true, quantization is done to int4. If false, quantization is done to uint4.\n                int4_op_types_to_quantize = MatMul/Gather: Specify op types to target for int4 quantization.\n                    Use this option when you want to quantize specific ops.\n                    Separate the op types with a '/' when passing them here (e.g. int4_op_types_to_quantize=MatMul/Gather)\n                int4_nodes_to_exclude = Specify nodes to exclude from int4 quantization. \n                    Use this option when you want to exclude certain nodes from being quantized.\n                    Separate the node names with a ',' when passing them here (e.g. int4_nodes_to_exclude=/lm_head/MatMul,/model/embed_tokens/Gather)\n                num_hidden_layers = Manually specify the number of layers in your ONNX model (for unit testing purposes).\n                filename = Filename for ONNX model (default is 'model.onnx').\n                    For models with multiple components, each component is exported to its own ONNX model.\n                    The filename for each component will be '<filename>_<component-name>.onnx' (ex: '<filename>_encoder.onnx', '<filename>_decoder.onnx').\n                config_only = Generate config and pre/post processing files only.\n                    Use this option when you already have your optimized and/or quantized ONNX model.\n                hf_token = false/token: Use this to manage authentication with Hugging Face.\n                    Default behavior is to use the authentication token stored by `huggingface-cli login`.\n                    If false, authentication with Hugging Face will be disabled.\n                    If token, you can provide a custom authentication token that differs from the one stored in your environment.\n                    If you have already authenticated via `huggingface-cli login`, you do not need to use this flag because Hugging Face has already stored your authentication token for you.\n                exclude_embeds = Remove embedding layer from your ONNX model.\n                    Use this option when you want to remove the embedding layer from within your ONNX model.\n                    Instead of `input_ids`, you will have `inputs_embeds` as the input to your ONNX model.\n                exclude_lm_head = Remove language modeling head from your ONNX model.\n                    Use this option when you want to remove the language modeling head from within your ONNX model.\n                    Instead of `logits`, you will have `hidden_states` as the output to your ONNX model.\n                include_hidden_states = Include hidden states as output from your ONNX model.\n                    Use this option when you want to have the hidden states as an output from your ONNX model.\n                    In addition to `logits`, you will have `hidden_states` as an output to your ONNX model.\n                enable_cuda_graph = Enable CUDA graph capture during inference. Default is false.\n                    If enabled, all nodes being placed on the CUDA EP is the prerequisite for the CUDA graph to be used correctly.\n                    It is not guaranteed that CUDA graph be enabled as it depends on the model and the graph structure.\n                use_8bits_moe = Use 8-bit quantization for MoE layers. Default is false.\n                    If true, the QMoE op will use 8-bit quantization. If false, the QMoE op will use 4-bit quantization.\n                use_qdq = Use the QDQ decomposition for ops.\n                    Use this option when you want to use quantize-dequantize ops. For example, you will have a quantized MatMul op instead of the MatMulNBits op.\n                use_webgpu_fp32 = Use FP32 for WebGPU EP.\n                    Use this option to enable GPUs that do not support FP16 on WebGPU (e.g. GTX 10xx).\n                adapter_path = Path to folder on disk containing the adapter files (adapter_config.json and adapter model weights).\n                    Use this option for LoRA models.\n                include_prompt_templates = Include prompt templates in the GenAI config file. Default is false.\n                    Use this option to include per-role prompt templates in the `genai_config.json` file.\n            "))
    args = parser.parse_args()
    print('Valid precision + execution provider combinations are: FP32 CPU, FP32 CUDA, FP16 CUDA, FP16 DML, INT4 CPU, INT4 CUDA, INT4 DML, INT4 WEBGPU')
    return args
if __name__ == '__main__':
    args = get_args()
    extra_options = parse_extra_options(args.extra_options)
    create_model(args.model_name, args.input, args.output, args.precision, args.execution_provider, args.cache_dir, **extra_options)
</code></pre>

<h2>onnx_progressive.py</h2>
<p>此脚本使用 ONNX Runtime 在 PC 端 (CPU Provider) 对转换后的 <code>gemma-3-1b-onnx-gqa</code> 模型进行自回归文本生成。它演示了如何加载 ONNX 模型、管理 KV 缓存、并逐 token 生成文本，同时依赖 <code>genai_config.json</code> 获取模型元信息。</p>
<pre class="long-code-block"><code class="language-python">
# onnx_progressive.py

import json
import numpy as np
import onnxruntime
from transformers import AutoTokenizer

# --- 用户配置 ---
model_dir = "./gemma-3-1b-it-onnx-cpu-fp32" # 替换为你的模型路径
execution_providers = ['CPUExecutionProvider'] # ['CUDAExecutionProvider', 'CPUExecutionProvider'] 或 ['CPUExecutionProvider']
messages = [
    { "role": "user", "content": "你好，请讲述一个深空探索的故事" },
]
max_new_tokens = 512 # 生成的最大新token数
# --- 结束用户配置 ---

print(f"模型路径: {model_dir}, 执行器: {execution_providers}")

# 1. 加载 Tokenizer 和配置
tokenizer = AutoTokenizer.from_pretrained(model_dir)
with open(f"{model_dir}/genai_config.json", 'r', encoding='utf-8') as f:
    genai_config = json.load(f)
print("Tokenizer 和 genai_config.json 加载成功。")

model_level_params = genai_config['model'] # model 级别的参数，如 bos, eos, pad_token_id, type, vocab_size
decoder_specific_params = model_level_params['decoder'] # decoder 特定的参数，如层数、头数等

onnx_model_filename = decoder_specific_params.get('filename', 'model.onnx')
onnx_model_path = f"{model_dir}/{onnx_model_filename}"
model_dtype = np.float16 if "fp16" in onnx_model_filename.lower() else np.float32
print(f"ONNX模型路径: {onnx_model_path}, 预期数据类型: {model_dtype}")

# 从 decoder_specific_params 获取模型结构参数
num_layers = decoder_specific_params['num_hidden_layers']
num_kv_heads = decoder_specific_params['num_key_value_heads']
head_size = decoder_specific_params['head_size']
print(f"模型结构: num_layers={num_layers}, num_kv_heads={num_kv_heads}, head_size={head_size}")


# 从 model_level_params 获取 token ID 相关参数
eos_token_id_from_config = model_level_params.get('eos_token_id', tokenizer.eos_token_id)
# 处理 eos_token_id 可能为单个int或list的情况
if isinstance(eos_token_id_from_config, list):
    eos_token_ids = eos_token_id_from_config
else:
    eos_token_ids = [eos_token_id_from_config]
eos_token_ids = [tid for tid in eos_token_ids if tid is not None] # 移除None
if not eos_token_ids: # 如果列表为空
    default_eos = getattr(tokenizer, 'eos_token_id', None)
    eos_token_ids = [default_eos] if default_eos is not None else [2] # 最终回退
print(f"使用的EOS token IDs: {eos_token_ids}")

# ONNX 输入/输出名称从 decoder_specific_params['inputs'] 和 ['outputs'] 获取
onnx_inputs_map = decoder_specific_params['inputs']
onnx_outputs_map = decoder_specific_params['outputs']

input_ids_name = onnx_inputs_map['input_ids']
# position_ids 可能不存在于某些GQA优化配置中，genai_config['model']['decoder']['inputs']中可能没有这个键
position_ids_name = onnx_inputs_map.get('position_ids', None) # 使用 .get() 避免KeyError
attention_mask_name = onnx_inputs_map['attention_mask']
past_key_input_tpl = onnx_inputs_map['past_key_names']
past_value_input_tpl = onnx_inputs_map['past_value_names']

logits_output_name = onnx_outputs_map['logits']
present_key_output_tpl = onnx_outputs_map['present_key_names']
present_value_output_tpl = onnx_outputs_map['present_value_names']

# 2. 加载 ONNX 模型会话
session_options = onnxruntime.SessionOptions()
# 从 genai_config.json 中获取 session_options (如果存在)
genai_session_options = decoder_specific_params.get('session_options', {})
log_id_from_config = genai_session_options.get('log_id')
if log_id_from_config:
    session_options.logid = log_id_from_config
# 注意: provider_options 通常在加载时直接传递给 InferenceSession 的 providers 参数，
# 或者需要更复杂的逻辑来解析并应用到 session_options.add_provider_options()

ort_session = onnxruntime.InferenceSession(
    onnx_model_path,
    sess_options=session_options,
    providers=execution_providers
)
print(f"ONNX模型加载成功，使用Provider: {ort_session.get_providers()}")

# 3. 准备初始输入
prompt_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
input_tokens = tokenizer(prompt_text, return_tensors="np")

current_input_ids = input_tokens['input_ids']
batch_size = current_input_ids.shape[0]
prompt_seq_len = current_input_ids.shape[1]

current_attention_mask = input_tokens.get('attention_mask', np.ones_like(current_input_ids, dtype=np.int64)).astype(np.int64)

# Position IDs: 初始绝对位置
# 只有当 position_ids_name 从 genai_config 中成功获取到（即不为None）时才创建
if position_ids_name:
    current_position_ids = np.arange(prompt_seq_len, dtype=np.int64).reshape(batch_size, prompt_seq_len)
else:
    current_position_ids = None # 明确表示不使用
    print("[信息] genai_config 中未定义 'position_ids' 输入，脚本将不创建或传递它。")


# 初始化 Past Key/Value (KV Cache)
past_kv_cache = {}
for i in range(num_layers):
    key_input_name = past_key_input_tpl.replace("%d", str(i))
    value_input_name = past_value_input_tpl.replace("%d", str(i))
    past_kv_cache[key_input_name] = np.zeros([batch_size, num_kv_heads, 0, head_size], dtype=model_dtype)
    past_kv_cache[value_input_name] = np.zeros([batch_size, num_kv_heads, 0, head_size], dtype=model_dtype)

# 4. 构建ONNX期望的输出名称列表
onnx_output_names = [logits_output_name]
for i in range(num_layers):
    onnx_output_names.append(present_key_output_tpl.replace("%d", str(i)))
    onnx_output_names.append(present_value_output_tpl.replace("%d", str(i)))

# 5. 生成循环
all_generated_token_ids = current_input_ids.copy()
print(f"\n--- 开始生成 (最多 {max_new_tokens} 个新 token) ---")
prompt_decoded = tokenizer.decode(current_input_ids[0], skip_special_tokens=False)
print(f"Prompt: {prompt_decoded}")
print("\nAssistant: ", end='', flush=True)

current_seq_len = prompt_seq_len

for step in range(max_new_tokens):
    ort_inputs = {
        input_ids_name: current_input_ids,
        attention_mask_name: current_attention_mask,
    }

    # 只有当 position_ids_name 有效 且 current_position_ids 已创建时才添加
    if position_ids_name and current_position_ids is not None:
        ort_inputs[position_ids_name] = current_position_ids
    elif step == 0 and position_ids_name is None: # 仅在第一步打印，如果genai_config没有position_ids
        pass # 上面已经打印过信息了
    elif step == 0 and current_position_ids is None and position_ids_name is not None:
        print(f"[警告] genai_config 中定义了 '{position_ids_name}' 但脚本未能创建它。")


    ort_inputs.update(past_kv_cache)

    outputs = ort_session.run(onnx_output_names, ort_inputs)
    logits, present_kv_states = outputs[0], outputs[1:]

    next_token_logits = logits[:, -1, :]
    next_token_id = np.argmax(next_token_logits, axis=-1, keepdims=True).astype(np.int64)

    token_text = tokenizer.decode(next_token_id[0], skip_special_tokens=True)
    print(token_text, end='', flush=True)

    all_generated_token_ids = np.concatenate([all_generated_token_ids, next_token_id], axis=-1)

    if next_token_id[0, 0] in eos_token_ids:
        print(f"\n[信息] 检测到EOS token ID: {next_token_id[0,0]}. 停止。")
        break

    current_input_ids = next_token_id
    if position_ids_name and current_position_ids is not None: # 仅当模型需要时才更新
        current_position_ids = np.array([[current_seq_len]], dtype=np.int64) # 下一个token的绝对位置

    current_attention_mask = np.concatenate(
        [current_attention_mask, np.ones((batch_size, 1), dtype=np.int64)],
        axis=-1
    )
    current_seq_len += 1

    new_past_kv_cache = {}
    for i_layer in range(num_layers):
        key_input_name = past_key_input_tpl.replace("%d", str(i_layer))
        value_input_name = past_value_input_tpl.replace("%d", str(i_layer))
        new_past_kv_cache[key_input_name] = present_kv_states[i_layer * 2]
        new_past_kv_cache[value_input_name] = present_kv_states[i_layer * 2 + 1]
    past_kv_cache = new_past_kv_cache

    if step == max_new_tokens - 1:
        print(f"\n[信息] 达到最大生成长度: {max_new_tokens}，停止。")
        break

print("\n--- 生成结束 ---")
</code></pre>